% intrinsic dimension

@ARTICLE{Denti2021-jl,
  title         = "Distributional Results for {Model-Based} Intrinsic Dimension
                   Estimators",
  author        = "Denti, Francesco and Doimo, Diego and Laio, Alessandro and
                   Mira, Antonietta",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "2104.13832"
}


% tSNE and UMAP

@ARTICLE{Van_der_Maaten2008-dv,
  title     = "Visualizing Data using {t-SNE}",
  author    = "Van der Maaten, Laurens and Hinton, Geoffrey",
  abstract  = "We present a new technique called 't-SNE' that visualizes
               high-dimensional data by giving each datapoint a location in a
               two or three-dimensional map. The technique is a variation of
               Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is
               much easier to optimize ...",
  journal   = "Journal of Machine Learning Research",
  publisher = "jmlr.org",
  volume    =  9,
  number    = "Nov",
  pages     = "2579--2605",
  year      =  2008
}

@ARTICLE{McInnes2018-xo,
  title         = "{UMAP}: Uniform Manifold Approximation and Projection for
                   Dimension Reduction",
  author        = "McInnes, Leland and Healy, John and Melville, James",
  month         =  feb,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1802.03426"
}

# Efficient computation of NN
@INPROCEEDINGS{Dong2011-ee,
  title     = "Efficient k-nearest neighbor graph construction for generic
               similarity measures",
  booktitle = "Proceedings of the 20th international conference on World wide
               web",
  author    = "Dong, Wei and Moses, Charikar and Li, Kai",
  publisher = "Association for Computing Machinery",
  pages     = "577--586",
  series    = "WWW '11",
  month     =  mar,
  year      =  2011,
  address   = "New York, NY, USA",
  keywords  = "arbitrary similarity measure, iterative method, k-nearest
               neighbor graph",
  location  = "Hyderabad, India"
}

@ARTICLE{Van_Der_Maaten2014-in,
  title     = "Accelerating {t-SNE} using tree-based algorithms",
  author    = "Van Der Maaten, Laurens",
  journal   = "Journal of Machine Learning Research",
  publisher = "JMLR.org",
  volume    =  15,
  number    =  1,
  pages     = "3221--3245",
  year      =  2014
}

@INPROCEEDINGS{Tang2016-ho,
  title     = "Visualizing Large-scale and High-dimensional Data",
  booktitle = "Proceedings of the 25th International Conference on World Wide
               Web",
  author    = "Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei, Qiaozhu",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "287--297",
  series    = "WWW '16",
  month     =  apr,
  year      =  2016,
  address   = "Republic and Canton of Geneva, CHE",
  keywords  = "visualization, big data, high-dimensional data",
  location  = "Montr{\'e}al, Qu{\'e}bec, Canada"
}


@ARTICLE{Talagala2020-ck,
  title     = "Anomaly Detection in Streaming Nonstationary Temporal Data",
  author    = "Talagala, Priyanga Dilini and Hyndman, Rob J and Smith-Miles,
               Kate and Kandanaarachchi, Sevvandi and Mu{\~n}oz, Mario A",
  abstract  = "AbstractThis article proposes a framework that provides early
               detection of anomalous series within a large collection of
               nonstationary streaming time-series data. We define an anomaly
               as an observation, that is, very unlikely given the recent
               distribution of a given system. The proposed framework first
               calculates a boundary for the system?s typical behavior using
               extreme value theory. Then a sliding window is used to test for
               anomalous series within a newly arrived collection of series.
               The model uses time series features as inputs, and a
               density-based comparison to detect any significant changes in
               the distribution of the features. Using various synthetic and
               real world datasets, we demonstrate the wide applicability and
               usefulness of our proposed framework. We show that the proposed
               algorithm can work well in the presence of noisy nonstationarity
               data within multiple classes of time series. This framework is
               implemented in the open source R package oddstream. R code and
               data are available in the online supplementary materials.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    =  1,
  pages     = "13--27",
  month     =  jan,
  year      =  2020
}

@ARTICLE{Shepard1962-ft,
  title     = "The analysis of proximities: Multidimensional scaling with an
               unknown distance function. {II}",
  author    = "Shepard, Roger N",
  abstract  = "The first in the present series of two papers described a
               computer program for multidimensional scaling on the basis of
               essentially nonmetric data. This second paper reports the
               results of two kinds of test applications of that program. The
               first application is to artificial data generated by
               monotonically transforming the interpoint distances in a known
               spatial configuration. The purpose is to show that the recovery
               of the original metric configuration does not depend upon the
               particular transformation used. The second application is to
               measures of interstimulus similarity and confusability obtained
               from some actual psychological experiments.",
  journal   = "Psychometrika",
  publisher = "Springer",
  volume    =  27,
  number    =  3,
  pages     = "219--246",
  month     =  sep,
  year      =  1962
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Lee2007-qa,
  title       = "Dimensionality reduction and clustering on statistical
                 manifolds",
  booktitle   = "2007 {IEEE} Conference on Computer Vision and Pattern
                 Recognition",
  author      = "Lee, Sang-Mook and Abbott, A Lynn and Araman, Philip A",
  abstract    = "Dimensionality reduction and clustering on statistical
                 manifolds is presented. Statistical manifold [16] is a 2D
                 Riemannian manifold which is statistically defined by maps
                 that transform a parameter domain onto a set of probability
                 density functions. Principal component analysis (PCA) based
                 dimensionality reduction is performed on the manifold, and
                 therefore, estimation of a mean and a variance of the set of
                 probability distributions are needed. First, the probability
                 distributions are transformed by an isometric transform that …",
  publisher   = "ieeexplore.ieee.org",
  pages       = "1--7",
  institution = "IEEE",
  year        =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{LeCam1973-da,
  title     = "Convergence of estimates under dimensionality restrictions",
  author    = "LeCam, Lucien and {Others}",
  abstract  = "Consider independent identically distributed observations whose
               distribution depends on a parameter $\theta $. Measure the
               distance between two parameter points $\theta_1,\theta_2 $ by
               the Hellinger distance $ h (\theta_1,\theta_2) $. Suppose that
               for $ n $ observations there is a good but not perfect test of
               $\theta_0 $ against $\theta_n $. Then $ n^\{\frac \{1\}\{2\}\} h
               (\theta_0,\theta_n) $ stays away from zero and infinity. The
               usual parametric examples, regular or irregular, also have the
               property that there are estimates $\hat \{\theta\} _n $ such …",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  1,
  number    =  1,
  pages     = "38--53",
  year      =  1973
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hyndman2018-ia,
  title     = "Visualizing big energy data: Solutions for this crucial
               component of data analysis",
  author    = "Hyndman, R J and Liu, X and Pinson, P",
  abstract  = "Visualization is a crucial component of data analysis. It is
               always a good idea to plot the data before fitting models,
               making predictions, or drawing conclusions. As sensors of the
               electric grid are collecting large volumes of data from various
               sources, power industry professionals are facing the challenge
               of visualizing such data in a timely fashion. In this article,
               we demonstrate several data-visualization solutions for big
               energy data through three case studies involving smart-meter
               data, phasor measurement unit (PMU) data, and probabilistic …",
  journal   = "IEEE Power Energ. Mag.",
  publisher = "ieeexplore.ieee.org",
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Floyd1962-hx,
  title     = "Algorithm 97: Shortest path",
  author    = "Floyd, Robert W",
  abstract  = "\textcent{} onlment This procedure will perform different order
               arithmetic operations with b and c, putting the result in a. The
               order of the operation is given by op. For op= 1 addition is
               performed. For op= 2 multiplicaLion, repeated addition, is done.
               Beyond these the …",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  5,
  number    =  6,
  pages     = "345",
  month     =  jun,
  year      =  1962,
  address   = "New York, NY, USA"
}

@ARTICLE{Cook1995-jx,
  title     = "Grand Tour and Projection Pursuit",
  author    = "Cook, Dianne and Buja, Andreas and Cabrera, Javier and Hurley,
               Catherine",
  abstract  = "Abstract The grand tour and projection pursuit are two methods
               for exploring multivariate data. We show how to combine them
               into a dynamic graphical tool for exploratory data analysis,
               called a projection pursuit guided tour. This tool assists in
               clustering data when clusters are oddly shaped and in finding
               general low-dimensional structure in high-dimensional, and in
               particular, sparse data. An example shows that the method, which
               is projection-based, can be quite powerful in situations that
               may cause grief for methods based on kernel smoothing. The
               projection pursuit guided tour is also useful for comparing and
               developing projection pursuit indexes and illustrating some
               types of asymptotic results.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  4,
  number    =  3,
  pages     = "155--172",
  month     =  sep,
  year      =  1995
}

@ARTICLE{Martin2011-bm,
  title   = "Electricity smart metering customer behaviour trials findings
             report, Technical report",
  author  = "Martin, G",
  journal = "CER Commission for Energy Regulation",
  pages   = "1--146",
  year    =  2011
}

@misc{cer2012-data,
  title = "CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010 [dataset]",
  author = "{{Commission for Energy Regulation (CER)}}",
  journal = "Irish Social Science Data Archive",
  year = 2012,
  edition = "1st Edition",
  note = "SN: 0012-00"
}

@BOOK{Amari2016-hk,
  title     = "Information Geometry and Its Applications",
  author    = "Amari, Shun-Ichi",
  abstract  = "This is the first comprehensive book on information geometry,
               written by the founder of the field. It begins with an
               elementary introduction to dualistic geometry and proceeds to a
               wide range of applications, covering information science,
               engineering, and neuroscience. It consists of four parts, which
               on the whole can be read independently. A manifold with a
               divergence function is first introduced, leading directly to
               dualistic structure, the heart of information geometry. This
               part (Part I) can be apprehended without any knowledge of
               differential geometry. An intuitive explanation of modern
               differential geometry then follows in Part II, although the book
               is for the most part understandable without modern differential
               geometry. Information geometry of statistical inference,
               including time series analysis and semiparametric estimation
               (the Neyman--Scott problem), is demonstrated concisely in Part
               III. Applications addressed in Part IV include hot current
               topics in machine learning, signal processing, optimization, and
               neural networks. The book is interdisciplinary, connecting
               mathematics, information sciences, physics, and neurosciences,
               inviting readers to a new world of information and geometry.
               This book is highly recommended to graduate students and
               researchers who seek new mathematical methods and tools useful
               in their own fields.",
  publisher = "Springer",
  month     =  feb,
  year      =  2016,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Arya1998-bv,
  title     = "An optimal algorithm for approximate nearest neighbor searching
               fixed dimensions",
  author    = "Arya, Sunil and Mount, David M and Netanyahu, Nathan S and
               Silverman, Ruth and Wu, Angela Y",
  abstract  = "Consider a set of S of n data points in real d-dimensional
               space, Rd, where distances are measured using any Minkowski
               metric. In nearest neighbor searching, we preprocess S into a
               data structure, so that given any query point q$\in$ Rd, is the
               closest point of S to q can be reported quickly. Given any
               positive real ϵ, data point p is a (1 +ϵ)-approximate nearest
               neighbor of q if its distance from q is within a factor of (1 +
               ϵ) of the distance to the true nearest neighbor. We show that it
               is possible to preprocess a set of n points in Rd in O(dn log n)
               time and O(dn) space, so that given a query point q $\in$ Rd,
               and ϵ > 0, a (1 + ϵ)-approximate nearest neighbor of q can be
               computed in O(cd, ϵ log n) time, where cd,ϵ$\leq$d ⌈1 + 6d/ϵ⌉d
               is a factor depending only on dimension and ϵ. In general, we
               show that given an integer k $\geq$ 1, (1 + ϵ)-approximations to
               the k nearest neighbors of q can be computed in additional O(kd
               log n) time.",
  journal   = "Journal of the ACM",
  publisher = "Association for Computing Machinery",
  volume    =  45,
  number    =  6,
  pages     = "891--923",
  month     =  nov,
  year      =  1998,
  address   = "New York, NY, USA",
  keywords  = "approximation algorithms, post-office problem, closet-point
               queries, box-decomposition trees, priority search, nearest
               neighbor searching"
}

@ARTICLE{Laa2020-qx,
  title     = "A Slice Tour for Finding Hollowness in {High-Dimensional} Data",
  author    = "Laa, Ursula and Cook, Dianne and Valencia, German",
  abstract  = "AbstractTaking projections of high-dimensional data is a common
               analytical and visualization technique in statistics for working
               with high-dimensional problems. Sectioning, or slicing, through
               high dimensions is less common, but can be useful for
               visualizing data with concavities, or nonlinear structure. It is
               associated with conditional distributions in statistics, and
               also linked brushing between plots in interactive data
               visualization. This short technical note describes a simple
               approach for slicing in the orthogonal space of projections
               obtained when running a tour, thus presenting the viewer with an
               interpolated sequence of sliced projections. The method has been
               implemented in R as an extension to the tourr package, and can
               be used to explore for concave and nonlinear structures in
               multivariate distributions. Supplementary materials for this
               article are available online.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    =  3,
  pages     = "681--687",
  month     =  jul,
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Muja2009-de,
  title     = "Fast approximate nearest neighbors with automatic algorithm
               configuration",
  author    = "Muja, Marius and Lowe, David G",
  abstract  = "For many computer vision problems, the most time consuming
               component consists of nearest neighbor matching in
               high-dimensional spaces. There are no known exact algorithms for
               solving these high-dimensional problems that are faster than
               linear search. Approximate algorithms are known to provide large
               speedups with only minor loss in accuracy, but many such
               algorithms have been published with only minimal guidance on
               selecting an algorithm and its parameters for any given problem.
               In this paper, we describe a …",
  journal   = "VISAPP (1)",
  publisher = "lear.inrialpes.fr",
  volume    =  2,
  number    = "331-340",
  pages     = "2",
  year      =  2009
}

@ARTICLE{McQueen2016-xz,
  title    = "Megaman: Scalable Manifold Learning in Python",
  author   = "McQueen, James and Meil{\u a}, Marina and VanderPlas, Jacob and
              Zhang, Zhongyue",
  journal  = "J. Mach. Learn. Res.",
  volume   =  17,
  number   =  148,
  pages    = "1--5",
  year     =  2016
}

@ARTICLE{Lunga2014-kc,
  title     = "{Manifold-Learning-Based} Feature Extraction for Classification
               of Hyperspectral Data: A Review of Advances in Manifold Learning",
  author    = "Lunga, D and Prasad, S and Crawford, M M and Ersoy, O",
  abstract  = "Advances in hyperspectral sensing provide new capability for
               characterizing spectral signatures in a wide range of physical
               and biological systems, while inspiring new methods for
               extracting information from these data. HSI data often lie on
               sparse, nonlinear manifolds whose geometric and topological
               structures can be exploited via manifold-learning techniques. In
               this article, we focused on demonstrating the opportunities
               provided by manifold learning for classification of remotely
               sensed data. However, limitations and opportunities remain both
               for research and applications. Although these methods have been
               demonstrated to mitigate the impact of physical effects that
               affect electromagnetic energy traversing the atmosphere and
               reflecting from a target, nonlinearities are not always
               exhibited in the data, particularly at lower spatial
               resolutions, so users should always evaluate the inherent
               nonlinearity in the data. Manifold learning is data driven, and
               as such, results are strongly dependent on the characteristics
               of the data, and one method will not consistently provide the
               best results. Nonlinear manifold-learning methods require
               parameter tuning, although experimental results are typically
               stable over a range of values, and have higher computational
               overhead than linear methods, which is particularly relevant for
               large-scale remote sensing data sets. Opportunities for
               advancing manifold learning also exist for analysis of
               hyperspectral and multisource remotely sensed data. Manifolds
               are assumed to be inherently smooth, an assumption that some
               data sets may violate, and data often contain classes whose
               spectra are distinctly different, resulting in multiple
               manifolds or submanifolds that cannot be readily integrated with
               a single manifold representation. Developing appropriate
               characterizations that exploit the unique characteristics of
               these submanifolds for a particular data set is an open research
               problem for which hierarchical manifold structures appear to
               have merit. To date, most work in manifold learning has focused
               on feature extraction from single images, assuming stationarity
               across the scene. Research is also needed in joint exploitation
               of global and local embedding methods in dynamic, multitemporal
               environments and integration with semisupervised and active
               learning.",
  journal   = "IEEE Signal Process. Mag.",
  publisher = "ieeexplore.ieee.org",
  volume    =  31,
  number    =  1,
  pages     = "55--66",
  month     =  jan,
  year      =  2014,
  keywords  = "feature extraction;geophysics computing;learning (artificial
               intelligence);pattern classification;remote
               sensing;manifold-learning-based feature extraction;hyperspectral
               data classification;hyperspectral sensing;geometric
               structures;topological structures;parameter tuning;large-scale
               remote sensing data sets;Learning systems;Feature
               extraction;Hyperspectral imaging;Signal processing
               algorithms;Geometry;Laplace equations"
}

@ARTICLE{Carter2009-ti,
  title    = "{FINE}: fisher information nonparametric embedding",
  author   = "Carter, Kevin M and Raich, Raviv and Finn, William G and Hero,
              3rd, Alfred O",
  abstract = "We consider the problems of clustering, classification, and
              visualization of high-dimensional data when no straightforward
              euclidean representation exists. In this paper, we propose using
              the properties of information geometry and statistical manifolds
              in order to define similarities between data sets using the
              Fisher information distance. We will show that this metric can be
              approximated using entirely nonparametric methods, as the
              parameterization and geometry of the manifold is generally
              unknown. Furthermore, by using multidimensional scaling methods,
              we are able to reconstruct the statistical manifold in a
              low-dimensional euclidean space; enabling effective learning on
              the data. As a whole, we refer to our framework as Fisher
              Information Nonparametric Embedding (FINE) and illustrate its
              uses on practical problems, including a biomedical application
              and document classification.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  31,
  number   =  11,
  pages    = "2093--2098",
  month    =  nov,
  year     =  2009,
  language = "en"
}

@ARTICLE{Hellinger_undated-rs,
  title     = "Neue Begr{\"u}ndung der Theorie quadratischer Formen von
               unendlichvielen Ver{\"a}nderlichen",
  author    = "Hellinger, E",
  journal   = "J. Reine Angew. Math.",
  publisher = "De Gruyter",
  year      =  1909,
  number    =  136,
  pages     = "210--271",
  address   = "Berlin, Boston"
}

@ARTICLE{Malkov2020-jp,
  title    = "Efficient and Robust Approximate Nearest Neighbor Search Using
              Hierarchical Navigable Small World Graphs",
  author   = "Malkov, Yu A and Yashunin, D A",
  abstract = "We present a new approach for the approximate K-nearest neighbor
              search based on navigable small world graphs with controllable
              hierarchy (Hierarchical NSW, HNSW). The proposed solution is
              fully graph-based, without any need for additional search
              structures (typically used at the coarse search stage of the most
              proximity graph techniques). Hierarchical NSW incrementally
              builds a multi-layer structure consisting of a hierarchical set
              of proximity graphs (layers) for nested subsets of the stored
              elements. The maximum layer in which an element is present is
              selected randomly with an exponentially decaying probability
              distribution. This allows producing graphs similar to the
              previously studied Navigable Small World (NSW) structures while
              additionally having the links separated by their characteristic
              distance scales. Starting the search from the upper layer
              together with utilizing the scale separation boosts the
              performance compared to NSW and allows a logarithmic complexity
              scaling. Additional employment of a heuristic for selecting
              proximity graph neighbors significantly increases performance at
              high recall and in case of highly clustered data. Performance
              evaluation has demonstrated that the proposed general metric
              space search index is able to strongly outperform previous
              opensource state-of-the-art vector-only approaches. Similarity of
              the algorithm to the skip list structure allows straightforward
              balanced distributed implementation.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  42,
  number   =  4,
  pages    = "824--836",
  month    =  apr,
  year     =  2020,
  language = "en"
}

@ARTICLE{Aumuller2020-nk,
  title    = "{ANN-Benchmarks}: A benchmarking tool for approximate nearest
              neighbor algorithms",
  author   = "Aum{\"u}ller, Martin and Bernhardsson, Erik and Faithfull,
              Alexander",
  abstract = "This paper describes ANN-Benchmarks, a tool for evaluating the
              performance of in-memory approximate nearest neighbor algorithms.
              It provides a standard interface for measuring the performance
              and quality achieved by nearest neighbor algorithms on different
              standard data sets. It supports several different ways of
              integrating k-NN algorithms, and its configuration system
              automatically tests a range of parameter settings for each
              algorithm. Algorithms are compared with respect to many different
              (approximate) quality measures, and adding more is easy and fast;
              the included plotting front-ends can visualize these as images,
              LaTeX plots, and websites with interactive plots. ANN-Benchmarks
              aims to provide a constantly updated overview of the current
              state of the art of k-NN algorithms. In the short term, this
              overview allows users to choose the correct k-NN algorithm and
              parameters for their similarity search task; in the longer term,
              algorithm designers will be able to use this overview to test and
              refine automatic parameter tuning. The paper gives an overview of
              the system, evaluates the results of the benchmark, and points
              out directions for future work. Interestingly, very different
              approaches to k-NN search yield comparable quality-performance
              trade-offs. The system is available at http://ann-benchmarks.com.",
  journal  = "Information Systems",
  volume   =  87,
  pages    = "101374",
  month    =  jan,
  year     =  2020,
  keywords = "Benchmarking; Nearest neighbor search; Evaluation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bentley1975-zo,
  title     = "Multidimensional binary search trees used for associative
               searching",
  author    = "Bentley, Jon Louis",
  abstract  = "This paper develops the multidimensional binary search tree (or
               kd tree, where k is the dimensionality of the search space) as a
               data structure for storage of information to be retrieved by
               associative searches. The kd tree is defined and examples are
               given. It is shown to be quite efficient in its storage
               requirements. A significant advantage of this structure is that
               a single data structure can handle many types of queries very
               efficiently. Various utility algorithms are developed; their
               proven average running times in an n record …",
  journal   = "Communications of the ACM",
  publisher = "Association for Computing Machinery",
  volume    =  18,
  number    =  9,
  pages     = "509--517",
  month     =  sep,
  year      =  1975,
  address   = "New York, NY, USA",
  keywords  = "information retrieval system, attribute, associative retrieval,
               binary search trees, partial match queries, intersection
               queries, key, nearest neighbor queries, binary tree insertion"
}

@BOOK{Lee2007-wq,
  title     = "Nonlinear Dimensionality Reduction",
  author    = "Lee, John A and Verleysen, Michel",
  abstract  = "Methods of dimensionality reduction provide a way to understand
               and visualize the structure of complex data sets. Traditional
               methods like principal component analysis and classical metric
               multidimensional scaling suffer from being based on linear
               models. Until recently, very few methods were able to reduce the
               data dimensionality in a nonlinear way. However, since the late
               nineties, many new methods have been developed and nonlinear
               dimensionality reduction, also called manifold learning, has
               become a hot topic. New advances that account for this rapid
               growth are, e.g. the use of graphs to represent the manifold
               topology, and the use of new metrics like the geodesic distance.
               In addition, new optimization schemes, based on kernel
               techniques and spectral decomposition, have lead to spectral
               embedding, which encompasses many of the recently developed
               methods. This book describes existing and advanced methods to
               reduce the dimensionality of numerical databases. For each
               method, the description starts from intuitive ideas, develops
               the necessary mathematical details, and ends by outlining the
               algorithmic implementation. Methods are compared with each other
               with the help of different illustrative examples. The purpose of
               the book is to summarize clear facts and ideas about well-known
               methods as well as recent developments in the topic of nonlinear
               dimensionality reduction. With this goal in mind, methods are
               all described from a unifying point of view, in order to
               highlight their respective strengths and shortcomings. The book
               is primarily intended for statisticians, computer scientists and
               data analysts. It is also accessible to other practitioners
               having a basic background in statistics and/or computational
               learning, like psychologists (in psychometry) and economists.",
  publisher = "Springer Science \& Business Media",
  month     =  oct,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Chen2009-su,
  title     = "Local Multidimensional Scaling for Nonlinear Dimension
               Reduction, Graph Drawing, and Proximity Analysis",
  author    = "Chen, Lisha and Buja, Andreas",
  abstract  = "In the past decade there has been a resurgence of interest in
               nonlinear dimension reduction. Among new proposals are ?Local
               Linear Embedding,? ?Isomap,? and Kernel Principal Components
               Analysis which all construct global low-dimensional embeddings
               from local affine or metric information. We introduce a
               competing method called ?Local Multidimensional Scaling? (LMDS).
               Like LLE, Isomap, and KPCA, LMDS constructs its global embedding
               from local information, but it uses instead a combination of MDS
               and ?force-directed? graph drawing. We apply the force paradigm
               to create localized versions of MDS stress functions with a
               tuning parameter to adjust the strength of nonlocal repulsive
               forces. We solve the problem of tuning parameter selection with
               a meta-criterion that measures how well the sets of K-nearest
               neighbors agree between the data and the embedding. Tuned LMDS
               seems to be able to outperform MDS, PCA, LLE, Isomap, and KPCA,
               as illustrated with two well-known image datasets. The
               meta-criterion can also be used in a pointwise version as a
               diagnostic tool for measuring the local adequacy of embeddings
               and thereby detect local problems in dimension reductions.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  104,
  number    =  485,
  pages     = "209--219",
  month     =  mar,
  year      =  2009
}

@ARTICLE{Venna2006-nd,
  title    = "Local multidimensional scaling",
  author   = "Venna, Jarkko and Kaski, Samuel",
  abstract = "In a visualization task, every nonlinear projection method needs
              to make a compromise between trustworthiness and continuity. In a
              trustworthy projection the visualized proximities hold in the
              original data as well, whereas a continuous projection visualizes
              all proximities of the original data. We show experimentally that
              one of the multidimensional scaling methods, curvilinear
              components analysis, is good at maximizing trustworthiness. We
              then extend it to focus on local proximities both in the input
              and output space, and to explicitly make a user-tunable
              parameterized compromise between trustworthiness and continuity.
              The new method compares favorably to alternative nonlinear
              projection methods.",
  journal  = "Neural Netw.",
  volume   =  19,
  number   = "6-7",
  pages    = "889--899",
  month    =  jul,
  year     =  2006,
  language = "en"
}

@ARTICLE{Goldberg2009-tb,
  title    = "Local procrustes for manifold embedding: a measure of embedding
              quality and embedding algorithms",
  author   = "Goldberg, Yair and Ritov, Ya'acov",
  abstract = "We present the Procrustes measure, a novel measure based on
              Procrustes rotation that enables quantitative comparison of the
              output of manifold-based embedding algorithms such as LLE (Roweis
              and Saul, Science 290(5500), 2323--2326, 2000) and Isomap
              (Tenenbaum et al., Science 290(5500), 2319--2323, 2000). The
              measure also serves as a natural tool when choosing
              dimension-reduction parameters. We also present two novel
              dimension-reduction techniques that attempt to minimize the
              suggested measure, and compare the results of these techniques to
              the results of existing algorithms. Finally, we suggest a simple
              iterative method that can be used to improve the output of
              existing algorithms.",
  journal  = "Mach. Learn.",
  volume   =  77,
  number   =  1,
  pages    = "1--25",
  month    =  oct,
  year     =  2009
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Friedman1977-dh,
  title     = "An Algorithm for Finding Best Matches in Logarithmic Expected
               Time",
  author    = "Friedman, Jerome H and Bentley, Jon Louis and Finkel, Raphael
               Ari",
  abstract  = "An algorithm and data structure are presented for searching a
               file containing N records, each described by k real valued keys,
               for the m closest matches or nearest neighbors to a given query
               record. The computation required to organize the file is
               proportional to kN log N …",
  journal   = "ACM Trans. Math. Softw.",
  publisher = "Association for Computing Machinery",
  volume    =  3,
  number    =  3,
  pages     = "209--226",
  month     =  sep,
  year      =  1977,
  address   = "New York, NY, USA"
}

@ARTICLE{Dijkstra1959-ml,
  title   = "A note on two problems in connexion with graphs",
  author  = "Dijkstra, E W",
  journal = "Numer. Math.",
  volume  =  1,
  number  =  1,
  pages   = "269--271",
  month   =  dec,
  year    =  1959
}

@ARTICLE{Coifman2006-no,
  title     = "Diffusion maps",
  author    = "Coifman, Ronald R and Lafon, St{\'e}phane",
  abstract  = "In this paper, we provide a framework based upon diffusion
               processes for finding meaningful geometric descriptions of data
               sets. We show that eigenfunctions of Markov matrices can be used
               to construct coordinates called diffusion maps that generate
               efficient representations of complex geometric structures. The
               associated family of diffusion distances, obtained by iterating
               the Markov matrix, defines multiscale geometries that prove to
               be useful in the context of data parametrization and
               dimensionality reduction. The proposed framework relates the
               spectral properties of Markov processes to their geometric
               counterparts and it unifies ideas arising in a variety of
               contexts such as machine learning, spectral graph theory and
               eigenmap methods.",
  journal   = "Appl. Comput. Harmon. Anal.",
  publisher = "Elsevier",
  volume    =  21,
  number    =  1,
  pages     = "5--30",
  month     =  jul,
  year      =  2006,
  keywords  = "Diffusion processes; Diffusion metric; Manifold learning;
               Dimensionality reduction; Eigenmaps; Graph Laplacian"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Nadler2006-cm,
  title     = "Diffusion Maps, Spectral Clustering and Eigenfunctions of
               {Fokker-Planck} Operators",
  booktitle = "Advances in Neural Information Processing Systems 18",
  author    = "Nadler, Boaz and Lafon, Stephane and Kevrekidis, Ioannis and
               Coifman, Ronald R",
  editor    = "Weiss, Y and Sch{\"o}lkopf, B and Platt, J C",
  abstract  = "This paper presents a diffusion based probabilistic
               interpretation of spectral clustering and dimensionality
               reduction algorithms that use the eigenvectors of the normalized
               graph Laplacian. Given the pairwise adjacency matrix of all
               points, we define a diffusion distance …",
  publisher = "MIT Press",
  pages     = "955--962",
  year      =  2006
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kraemer2018-zf,
  title     = "dimRed and {coRanking---Unifying} dimensionality reduction in
               {R}",
  author    = "Kraemer, Guido and Reichstein, Markus and Mahecha, Miguel D",
  abstract  = "Dimensionality reduction (DR) is a widely used approach to find
               low dimensional and interpretable representations of data that
               are natively embedded in high-dimensional spaces. DR can be
               realized by a plethora of methods with different properties,
               objectives …",
  journal   = "R J.",
  publisher = "R Foundation",
  volume    =  10,
  number    =  1,
  pages     = "342--358",
  year      =  2018
}

@ARTICLE{Weinberger2006-dc,
  title    = "Unsupervised Learning of Image Manifolds by Semidefinite
              Programming",
  author   = "Weinberger, Kilian Q and Saul, Lawrence K",
  abstract = "Can we detect low dimensional structure in high dimensional data
              sets of images? In this paper, we propose an algorithm for
              unsupervised learning of image manifolds by semidefinite
              programming. Given a data set of images, our algorithm computes a
              low dimensional representation of each image with the property
              that distances between nearby images are preserved. More
              generally, it can be used to analyze high dimensional data that
              lies on or near a low dimensional manifold. We illustrate the
              algorithm on easily visualized examples of curves and surfaces,
              as well as on actual images of faces, handwritten digits, and
              solid objects.",
  journal  = "International Journal of Computer Vision",
  volume   =  70,
  number   =  1,
  pages    = "77--90",
  month    =  oct,
  year     =  2006
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Donoho2003-am,
  title    = "Hessian eigenmaps: locally linear embedding techniques for
              high-dimensional data",
  author   = "Donoho, David L and Grimes, Carrie",
  abstract = "We describe a method for recovering the underlying
              parametrization of scattered data (m(i)) lying on a manifold M
              embedded in high-dimensional Euclidean space. The method,
              Hessian-based locally linear embedding, derives from a conceptual
              framework of local isometry in which the manifold M, viewed as a
              Riemannian submanifold of the ambient Euclidean Space R(n), is
              locally isometric to an open, connected subset $\Theta$ of
              Euclidean space R(d). Because $\Theta$ does not have to be
              convex, this framework is able to handle a significantly wider
              class of situations than the original ISOMAP algorithm. The
              theoretical framework revolves around a quadratic form H(f) =
              $\int$(M)||H(f)(m)||²(F)dm defined on functions f: M--> R. Here
              Hf denotes the Hessian of f, and H(f) averages the Frobenius norm
              of the Hessian over M. To define the Hessian, we use orthogonal
              coordinates on the tangent planes of M. The key observation is
              that, if M truly is locally isometric to an open, connected
              subset of R(d), then H(f) has a (d + 1)-dimensional null space
              consisting of the constant functions and a d-dimensional space of
              functions spanned by the original isometric coordinates. Hence,
              the isometric coordinates can be recovered up to a linear
              isometry. Our method may be viewed as a modification of locally
              linear embedding and our theoretical framework as a modification
              of the Laplacian eigenmaps framework, where we substitute a
              quadratic form based on the Hessian in place of one based on the
              Laplacian.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  100,
  number   =  10,
  pages    = "5591--5596",
  month    =  may,
  year     =  2003,
  language = "en"
}

@ARTICLE{Belkin2003-kz,
  title    = "Laplacian Eigenmaps for Dimensionality Reduction and Data
              Representation",
  author   = "Belkin, Mikhail and Niyogi, Partha",
  abstract = "Download Citation | Laplacian Eigenmaps for Dimensionality
              Reduction and Data Representation | One of the central problems
              in machine learning and pattern recognition is to develop
              appropriate representations for complex data. We consider... |
              Find, read and cite all the research you need on ResearchGate",
  journal  = "Neural Comput.",
  volume   =  15,
  number   =  6,
  pages    = "1373--1396",
  month    =  jun,
  year     =  2003
}

@ARTICLE{Hyndman1996-lk,
  title     = "Computing and Graphing Highest Density Regions",
  author    = "Hyndman, Rob J",
  abstract  = "[Many statistical methods involve summarizing a probability
               distribution by a region of the sample space covering a
               specified probability. One method of selecting such a region is
               to require it to contain points of relatively high density.
               Highest density regions are particularly useful for displaying
               multimodal distributions and, in such cases, may consist of
               several disjoint subsets-one for each local mode. In this paper,
               I propose a simple method for computing a highest density region
               from any given (possibly multivariate) density f(x) that is
               bounded and continuous in x. Several examples of the use of
               highest density regions in statistical graphics are also given.
               A new form of boxplot is proposed based on highest density
               regions; versions in one and two dimensions are given. Highest
               density regions in higher dimensions are also discussed and
               plotted.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  50,
  number    =  2,
  pages     = "120--126",
  year      =  1996
}


@INPROCEEDINGS{Lee2008-dy,
  title     = "Quality assessment of nonlinear dimensionality reduction based
               on K-ary neighborhoods",
  booktitle = "Proceedings of the Workshop on New Challenges for Feature
               Selection in Data Mining and Knowledge Discovery at {ECML/PKDD}
               2008",
  author    = "Lee, John and Verleysen, Michel",
  editor    = "Saeys, Yvan and Liu, Huan and Inza, I{\~n}aki and Wehenkel,
               Louis and Van de Pee, Yves",
  abstract  = "Nonlinear dimensionality reduction aims at providing
               low-dimensional representions of high-dimensional data sets.
               Many new methods have been recently proposed, but the question
               of their assessment and comparison remains open. This paper
               reviews some of the existing quality measures that are based on
               distance ranking and K-ary neighborhoods. In this context, the
               comparison of the ranks in the high- and low-dimensional spaces
               leads to the definition of the co-ranking matrix. Rank errors
               and concepts such as neighborhood intrusions and extrusions can
               be associated with different blocks of the co-ranking matrix.
               The considered quality criteria are then cast within this
               unifying framework and the blocks they involve are identified.
               The same framework allows us to propose simpler criteria, which
               quantify two aspects of the embedding, namely its overall
               quality and its tendency to favor either intrusions or
               extrusions. Eventually, a simple experiment illustrates the
               soundness of the approach.",
  publisher = "PMLR",
  volume    =  4,
  pages     = "21--35",
  series    = "Proceedings of Machine Learning Research",
  year      =  2008,
  address   = "Antwerp, Belgium"
}


@ARTICLE{Roweis2000-ni,
  title     = "Nonlinear dimensionality reduction by locally linear embedding",
  author    = "Roweis, S T and Saul, L K",
  abstract  = "Many areas of science depend on exploratory data analysis and
               visualization. The need to analyze large amounts of multivariate
               data raises the fundamental problem of dimensionality reduction:
               how to discover compact representations of high-dimensional
               data. Here, we introduce locally linear embedding (LLE), an
               unsupervised learning algorithm that computes low-dimensional,
               neighborhood-preserving embeddings of high-dimensional inputs.
               Unlike clustering methods for local dimensionality reduction,
               LLE maps its inputs into a single global coordinate system of
               lower dimensionality, and its optimizations do not involve local
               minima. By exploiting the local symmetries of linear
               reconstructions, LLE is able to learn the global structure of
               nonlinear manifolds, such as those generated by images of faces
               or documents of text.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  290,
  number    =  5500,
  pages     = "2323--2326",
  month     =  dec,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Tenenbaum2000-fr,
  title     = "A global geometric framework for nonlinear dimensionality
               reduction",
  author    = "Tenenbaum, J B and de Silva, V and Langford, J C",
  abstract  = "Scientists working with large volumes of high-dimensional data,
               such as global climate patterns, stellar spectra, or human gene
               distributions, regularly confront the problem of dimensionality
               reduction: finding meaningful low-dimensional structures hidden
               in their high-dimensional observations. The human brain
               confronts the same problem in everyday perception, extracting
               from its high-dimensional sensory inputs-30,000 auditory nerve
               fibers or 10(6) optic nerve fibers-a manageably small number of
               perceptually relevant features. Here we describe an approach to
               solving dimensionality reduction problems that uses easily
               measured local metric information to learn the underlying global
               geometry of a data set. Unlike classical techniques such as
               principal component analysis (PCA) and multidimensional scaling
               (MDS), our approach is capable of discovering the nonlinear
               degrees of freedom that underlie complex natural observations,
               such as human handwriting or images of a face under different
               viewing conditions. In contrast to previous algorithms for
               nonlinear dimensionality reduction, ours efficiently computes a
               globally optimal solution, and, for an important class of data
               manifolds, is guaranteed to converge asymptotically to the true
               structure.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  290,
  number    =  5500,
  pages     = "2319--2323",
  month     =  dec,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Izenman2012-mx,
  title     = "Introduction to manifold learning",
  author    = "Izenman, Alan Julian",
  abstract  = "Abstract A popular research area today in statistics and machine
               learning is that of manifold learning, which is related to the
               algorithmic techniques of dimensionality reduction. Manifold
               learning can be divided into linear and nonlinear methods.
               Linear methods, which have long been part of the statistician's
               toolbox for analyzing multivariate data, include principal
               component analysis (PCA) and multidimensional scaling (MDS).
               Recently, there has been a flurry of research activity on
               nonlinear manifold learning, which includes Isomap, local linear
               embedding, Laplacian eigenmaps, Hessian eigenmaps, and diffusion
               maps. Some of these techniques are nonlinear generalizations of
               the linear methods. The algorithmic process of most of these
               techniques consists of three steps: a nearest-neighbor search, a
               definition of distances or affinities between points (a key
               ingredient for the success of these methods), and an
               eigenproblem for embedding high-dimensional points into a lower
               dimensional space. This article gives us a brief survey of these
               new methods and indicates their strengths and weaknesses. WIREs
               Comput Stat 2012 doi: 10.1002/wics.1222 This article is
               categorized under: Statistical and Graphical Methods of Data
               Analysis > Dimension Reduction Statistical Learning and
               Exploratory Methods of the Data Sciences > Manifold Learning
               Statistical and Graphical Methods of Data Analysis >
               Multivariate Analysis",
  journal   = "WIREs Comp Stat",
  publisher = "Wiley Online Library",
  volume    =  4,
  number    =  5,
  pages     = "439--446",
  month     =  sep,
  year      =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cayton2005-dp,
  title     = "Algorithms for manifold learning",
  author    = "Cayton, Lawrence",
  abstract  = "Manifold learning is a popular recent approach to nonlinear
               dimensionality reduction. Algorithms for this task are based on
               the idea that the dimensionality of many data sets is only
               artificially high; though each data point consists of perhaps
               thousands of features, it",
  journal   = "Univ. of California at San Diego Tech. Rep",
  publisher = "cseweb.ucsd.edu",
  volume    =  12,
  number    = "1-17",
  pages     = "1",
  year      =  2005
}

@ARTICLE{Shepard1962-ac,
  title    = "The analysis of proximities: Multidimensional scaling with an
              unknown distance function. {I}",
  author   = "Shepard, Roger N",
  abstract = "A computer program is described that is designed to reconstruct
              the metric configuration of a set of points in Euclidean space on
              the basis of essentially nonmetric information about that
              configuration. A minimum set of Cartesian coordinates for the
              points is determined when the only available information
              specifies for each pair of those points---not the distance
              between them---but some unknown, fixed monotonic function of that
              distance. The program is proposed as a tool for reductively
              analyzing several types of psychological data, particularly
              measures of interstimulus similarity or confusability, by making
              explicit the multidimensional structure underlying such data.",
  journal  = "Psychometrika",
  volume   =  27,
  number   =  2,
  pages    = "125--140",
  month    =  jun,
  year     =  1962
}

@ARTICLE{Kruskal1964-md,
  title     = "Nonmetric multidimensional scaling: A numerical method",
  author    = "Kruskal, J B",
  abstract  = "We describe the numerical methods required in our approach to
               multi-dimensional scaling. The rationale of this approach has
               appeared previously.",
  journal   = "Psychometrika",
  publisher = "Springer-Verlag",
  volume    =  29,
  number    =  2,
  pages     = "115--129",
  month     =  jun,
  year      =  1964,
  language  = "en"
}

@ARTICLE{Kruskal1964-iv,
  title     = "Multidimensional scaling by optimizing goodness of fit to a
               nonmetric hypothesis",
  author    = "Kruskal, J B",
  abstract  = "Multidimensional scaling is the problem of representingn objects
               geometrically byn points, so that the interpoint distances
               correspond in some sense to experimental dissimilarities between
               objects. In just what sense distances and dissimilarities should
               correspond has been left rather vague in most approaches, thus
               leaving these approaches logically incomplete. Our fundamental
               hypothesis is that dissimilarities and distances are
               monotonically related. We define a quantitative, intuitively
               satisfying measure of goodness of fit to this hypothesis. Our
               technique of multidimensional scaling is to compute that
               configuration of points which optimizes the goodness of fit. A
               practical computer program for doing the calculations is
               described in a companion paper.",
  journal   = "Psychometrika",
  publisher = "Springer",
  volume    =  29,
  number    =  1,
  pages     = "1--27",
  month     =  mar,
  year      =  1964
}


@ARTICLE{Zhu2018-jw,
  title    = "Image reconstruction by domain-transform manifold learning",
  author   = "Zhu, Bo and Liu, Jeremiah Z and Cauley, Stephen F and Rosen,
              Bruce R and Rosen, Matthew S",
  abstract = "Image reconstruction is essential for imaging applications across
              the physical and life sciences, including optical and radar
              systems, magnetic resonance imaging, X-ray computed tomography,
              positron emission tomography, ultrasound imaging and radio
              astronomy. During image acquisition, the sensor encodes an
              intermediate representation of an object in the sensor domain,
              which is subsequently reconstructed into an image by an inversion
              of the encoding function. Image reconstruction is challenging
              because analytic knowledge of the exact inverse transform may not
              exist a priori, especially in the presence of sensor
              non-idealities and noise. Thus, the standard reconstruction
              approach involves approximating the inverse function with
              multiple ad hoc stages in a signal processing chain, the
              composition of which depends on the details of each acquisition
              strategy, and often requires expert parameter tuning to optimize
              reconstruction performance. Here we present a unified framework
              for image reconstruction-automated transform by manifold
              approximation (AUTOMAP)-which recasts image reconstruction as a
              data-driven supervised learning task that allows a mapping
              between the sensor and the image domain to emerge from an
              appropriate corpus of training data. We implement AUTOMAP with a
              deep neural network and exhibit its flexibility in learning
              reconstruction transforms for various magnetic resonance imaging
              acquisition strategies, using the same network architecture and
              hyperparameters. We further demonstrate that manifold learning
              during training results in sparse representations of domain
              transforms along low-dimensional data manifolds, and observe
              superior immunity to noise and a reduction in reconstruction
              artefacts compared with conventional handcrafted reconstruction
              methods. In addition to improving the reconstruction performance
              of existing acquisition methodologies, we anticipate that AUTOMAP
              and other learned reconstruction approaches will accelerate the
              development of new acquisition strategies across imaging
              modalities.",
  journal  = "Nature",
  volume   =  555,
  number   =  7697,
  pages    = "487--492",
  month    =  mar,
  year     =  2018,
  language = "en"
}


@ARTICLE{Banerjee2019-at,
  title   = "Spectral clustering via sparse graph structure learning with
             application to proteomic signaling networks in cancer",
  author  = "Banerjee, Sayantan and Akbani, Rehan and Baladandayuthapani,
             Veerabhadran",
  journal = "Computational Statistics \& Data Analysis",
  volume  =  132,
  pages   = "46--69",
  year    =  2019
}




@INPROCEEDINGS{Zhang2003-yi,
  title     = "Nonlinear Dimension Reduction via Local Tangent Space Alignment",
  booktitle = "Intelligent Data Engineering and Automated Learning",
  author    = "Zhang, Zhenyue and Zha, Hongyuan",
  abstract  = "In this paper we present a new algorithm for manifold learning
               and nonlinear dimension reduction. Based on a set of unorganized
               data points sampled with noise from the manifold, we represent
               the local geometry of the manifold using tangent spaces learned
               by fitting an affine subspace in a neighborhood of each data
               point. Those tangent spaces are aligned to give the internal
               global coordinates of the data points with respect to the
               underlying manifold by way of a partial eigendecomposition of
               the neighborhood connection matrix. We present a careful error
               analysis of our algorithm and show that the reconstruction
               errors are of second-order accuracy. Numerical experiments
               including 64-by-64 pixel face images are given to illustrate our
               algorithm.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "477--481",
  year      =  2003
}

% MNIST
@misc{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

% ANN C++ library
@misc{mount2010-ann,
  author = {Mount, David and Arya, Sunil},
  title = {ANN: A Library for Approximate Nearest Neighbor Searching},
  year = {2010},
  version = {1.1.3},
  howpublished = {\url{http://www.cs.umd.edu/~mount/ANN}},
  note = {Accessed on 2020-09-24}
}

% k-d tree github repo
@misc{jefferislab2019-l2,
  author = {Mount, David and Arya, Sunil and Kemp, Samuel E. and Jefferis, Gregory and Mülle, Kirill},
  title = {Fast Nearest Neighbour Search (Wraps ANN Library) Using
    L2 Metric},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jefferislab/RANN}},
  note = {Accessed on 2020-09-24}
}


% Annoy github repo
@misc{Bernhardsson2016-tf,
  author = {Spotify},
  title = {Annoy},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/github/open-source-survey}},
  note = {Accessed on 2020-09-24}
}

% Annoy slides
@misc{Bernhardsson2015-slides,
  author = {Bernhardsson, Erik},
  title = {ANN presentation},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/erikbern/ann-presentation}},
  note = {Accessed on 2020-09-24}
}
