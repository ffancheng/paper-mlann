% Python package megaman
@ARTICLE{McQueen2016-xz,
  title    = "Megaman: Scalable Manifold Learning in Python",
  author   = "McQueen, James and Meil{\u a}, Marina and VanderPlas, Jacob and
              Zhang, Zhongyue",
  journal  = "J. Mach. Learn. Res.",
  volume   =  17,
  number   =  148,
  pages    = "1--5",
  year     =  2016
}

% FLANN
@ARTICLE{Muja2009-de,
  title     = "Fast approximate nearest neighbors with automatic algorithm
               configuration",
  author    = "Muja, Marius and Lowe, David G",
  abstract  = "For many computer vision problems, the most time consuming
               component consists of nearest neighbor matching in
               high-dimensional spaces. There are no known exact algorithms for
               solving these high-dimensional problems that are faster than
               linear search. Approximate algorithms are known to provide large
               speedups with only minor loss in accuracy, but many such
               algorithms have been published with only minimal guidance on
               selecting an algorithm and its parameters for any given problem.
               In this paper, we describe a …",
  journal   = "VISAPP (1)",
  publisher = "lear.inrialpes.fr",
  volume    =  2,
  number    = "331-340",
  pages     = "2",
  year      =  2009
}


@ARTICLE{Carter2009-ti,
  title    = "{FINE}: fisher information nonparametric embedding",
  author   = "Carter, Kevin M and Raich, Raviv and Finn, William G and Hero,
              3rd, Alfred O",
  abstract = "We consider the problems of clustering, classification, and
              visualization of high-dimensional data when no straightforward
              euclidean representation exists. In this paper, we propose using
              the properties of information geometry and statistical manifolds
              in order to define similarities between data sets using the
              Fisher information distance. We will show that this metric can be
              approximated using entirely nonparametric methods, as the
              parameterization and geometry of the manifold is generally
              unknown. Furthermore, by using multidimensional scaling methods,
              we are able to reconstruct the statistical manifold in a
              low-dimensional euclidean space; enabling effective learning on
              the data. As a whole, we refer to our framework as Fisher
              Information Nonparametric Embedding (FINE) and illustrate its
              uses on practical problems, including a biomedical application
              and document classification.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  31,
  number   =  11,
  pages    = "2093--2098",
  month    =  nov,
  year     =  2009,
  language = "en"
}


@ARTICLE{Lunga2014-kc,
  title     = "{Manifold-Learning-Based} Feature Extraction for Classification
               of Hyperspectral Data: A Review of Advances in Manifold Learning",
  author    = "Lunga, D and Prasad, S and Crawford, M M and Ersoy, O",
  abstract  = "Advances in hyperspectral sensing provide new capability for
               characterizing spectral signatures in a wide range of physical
               and biological systems, while inspiring new methods for
               extracting information from these data. HSI data often lie on
               sparse, nonlinear manifolds whose geometric and topological
               structures can be exploited via manifold-learning techniques. In
               this article, we focused on demonstrating the opportunities
               provided by manifold learning for classification of remotely
               sensed data. However, limitations and opportunities remain both
               for research and applications. Although these methods have been
               demonstrated to mitigate the impact of physical effects that
               affect electromagnetic energy traversing the atmosphere and
               reflecting from a target, nonlinearities are not always
               exhibited in the data, particularly at lower spatial
               resolutions, so users should always evaluate the inherent
               nonlinearity in the data. Manifold learning is data driven, and
               as such, results are strongly dependent on the characteristics
               of the data, and one method will not consistently provide the
               best results. Nonlinear manifold-learning methods require
               parameter tuning, although experimental results are typically
               stable over a range of values, and have higher computational
               overhead than linear methods, which is particularly relevant for
               large-scale remote sensing data sets. Opportunities for
               advancing manifold learning also exist for analysis of
               hyperspectral and multisource remotely sensed data. Manifolds
               are assumed to be inherently smooth, an assumption that some
               data sets may violate, and data often contain classes whose
               spectra are distinctly different, resulting in multiple
               manifolds or submanifolds that cannot be readily integrated with
               a single manifold representation. Developing appropriate
               characterizations that exploit the unique characteristics of
               these submanifolds for a particular data set is an open research
               problem for which hierarchical manifold structures appear to
               have merit. To date, most work in manifold learning has focused
               on feature extraction from single images, assuming stationarity
               across the scene. Research is also needed in joint exploitation
               of global and local embedding methods in dynamic, multitemporal
               environments and integration with semisupervised and active
               learning.",
  journal   = "IEEE Signal Process. Mag.",
  publisher = "ieeexplore.ieee.org",
  volume    =  31,
  number    =  1,
  pages     = "55--66",
  month     =  jan,
  year      =  2014,
  keywords  = "feature extraction;geophysics computing;learning (artificial
               intelligence);pattern classification;remote
               sensing;manifold-learning-based feature extraction;hyperspectral
               data classification;hyperspectral sensing;geometric
               structures;topological structures;parameter tuning;large-scale
               remote sensing data sets;Learning systems;Feature
               extraction;Hyperspectral imaging;Signal processing
               algorithms;Geometry;Laplace equations"
}


% Total Variation distance
@ARTICLE{Vervaat1969-et,
  title     = "Upper bounds for the distance in total variation between the
               binomial or negative binomial and the Poisson distribution",
  author    = "Vervaat, W",
  abstract  = "In (1.2)((1.3)) we recognize the total variation of the
               difference between the (negative) binomial distribution function
               and the Poisson-distribution function with the same expectation.
               In order to get bounds for D, and D2 we will use the following
               general inequality which turned out to be very useful in [I] and
               [a]. Let F and G be two distribution functions and pa a-finite
               measure with respect to which F and G are absolutely continuous
               with densities Sand g, then IIF-Gll, the total variation of the
               signed measure FG, satisfies",
  journal   = "Stat. Neerl.",
  publisher = "John Wiley \& Sons, Ltd",
  volume    =  23,
  number    =  1,
  pages     = "79--86",
  month     =  mar,
  year      =  1969
}


% Hellinger distance
@ARTICLE{Hellinger_undated-rs,
  title     = "Neue Begr{\"u}ndung der Theorie quadratischer Formen von
               unendlichvielen Ver{\"a}nderlichen",
  author    = "Hellinger, E",
  journal   = "J. Reine Angew. Math.",
  publisher = "De Gruyter",
  year      =  1909,
  volume    =  1909,
  number    =  136,
  pages     = "210--271",
  address   = "Berlin, Boston"
}


% MNIST
@misc{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

% t-SNE

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Maaten2008-dw,
  title     = "Visualizing Data using {t-SNE}",
  author    = "Maaten, Laurens van der and Hinton, Geoffrey",
  abstract  = "We present a new technique called`` t - SNE '' that visualizes
               high-dimensional data by giving each datapoint a location in a
               two or three-dimensional map. The technique is a variation of
               Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is
               much easier to optimize …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  9,
  number    = "Nov",
  pages     = "2579--2605",
  year      =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Friedman1977-dh,
  title     = "An Algorithm for Finding Best Matches in Logarithmic Expected
               Time",
  author    = "Friedman, Jerome H and Bentley, Jon Louis and Finkel, Raphael
               Ari",
  abstract  = "An algorithm and data structure are presented for searching a
               file containing N records, each described by k real valued keys,
               for the m closest matches or nearest neighbors to a given query
               record. The computation required to organize the file is
               proportional to kN log N …",
  journal   = "ACM Trans. Math. Softw.",
  publisher = "Association for Computing Machinery",
  volume    =  3,
  number    =  3,
  pages     = "209--226",
  month     =  sep,
  year      =  1977,
  address   = "New York, NY, USA"
}

@ARTICLE{Finkel1974-xt,
  title     = "Quad trees a data structure for retrieval on composite keys",
  author    = "Finkel, R A and Bentley, J L",
  abstract  = "The quad tree is a data structure appropriate for storing
               information to be retrieved on composite keys. We discuss the
               specific case of two-dimensional retrieval, although the
               structure is easily generalised to arbitrary dimensions.
               Algorithms are given both for staightforward insertion and for a
               type of balanced insertion into quad trees. Empirical analyses
               show that the average time for insertion is logarithmic with the
               tree size. An algorithm for retrieval within regions is
               presented along with data from empirical studies which imply
               that searching is reasonably efficient. We define an optimized
               tree and present an algorithm to accomplish optimization in n
               log n time. Searching is guaranteed to be fast in optimized
               trees. Remaining problems include those of deletion from quad
               trees and merging of quad trees, which seem to be inherently
               difficult operations.",
  journal   = "Acta Inform.",
  publisher = "Springer",
  volume    =  4,
  number    =  1,
  pages     = "1--9",
  month     =  mar,
  year      =  1974
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bentley1975-zo,
  title     = "Multidimensional binary search trees used for associative
               searching",
  author    = "Bentley, Jon Louis",
  abstract  = "This paper develops the multidimensional binary search tree (or
               kd tree, where k is the dimensionality of the search space) as a
               data structure for storage of information to be retrieved by
               associative searches. The kd tree is defined and examples are
               given. It is shown to be quite efficient in its storage
               requirements. A significant advantage of this structure is that
               a single data structure can handle many types of queries very
               efficiently. Various utility algorithms are developed; their
               proven average running times in an n record …",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  18,
  number    =  9,
  pages     = "509--517",
  month     =  sep,
  year      =  1975,
  address   = "New York, NY, USA",
  keywords  = "information retrieval system, attribute, associative retrieval,
               binary search trees, partial match queries, intersection
               queries, key, nearest neighbor queries, binary tree insertion"
}

@ARTICLE{AryaSunil1998-nd,
  title     = "An optimal algorithm for approximate nearest neighbor searching
               fixed dimensions",
  author    = "Arya, Sunil and Mount, David M and Netanyahu, Nathan S and
               Silverman, Ruth and Wu, Angela Y",
  abstract  = "Consider a set of S of n data points in real d-dimensional
               space, Rd, where distances are measured using any Minkowski
               metric. In nearest neighbor searching, we preprocess S into a
               data structure, ...",
  journal   = "J. ACM",
  publisher = "ACM PUB27 New York, NY, USA",
  month     =  nov,
  year      =  1998,
  language  = "en"
}

% ball tree

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Omohundro1989-iu,
  title     = "Five balltree construction algorithms",
  author    = "Omohundro, Stephen M",
  abstract  = "Balltrees are simple geometric data structures with a wide range
               of practical applications to geometric learning tasks. In this
               report we compare 5 different algorithms for constructing
               balltrees from data. We study the trade-off between construction
               time and the quality of the constructed tree. Two of the
               algorithms are on-line, two construct the structures from the
               data set in a top down fashion, and one uses a bottom up
               approach. We empirically study the algorithms on random data
               drawn from eight different probability distributions
               representing …",
  publisher = "International Computer Science Institute Berkeley",
  year      =  1989
}

% ANN-Benchmark

@ARTICLE{Aumuller2020-nk,
  title    = "{ANN-Benchmarks}: A benchmarking tool for approximate nearest
              neighbor algorithms",
  author   = "Aum{\"u}ller, Martin and Bernhardsson, Erik and Faithfull,
              Alexander",
  abstract = "This paper describes ANN-Benchmarks, a tool for evaluating the
              performance of in-memory approximate nearest neighbor algorithms.
              It provides a standard interface for measuring the performance
              and quality achieved by nearest neighbor algorithms on different
              standard data sets. It supports several different ways of
              integrating k-NN algorithms, and its configuration system
              automatically tests a range of parameter settings for each
              algorithm. Algorithms are compared with respect to many different
              (approximate) quality measures, and adding more is easy and fast;
              the included plotting front-ends can visualize these as images,
              LaTeX plots, and websites with interactive plots. ANN-Benchmarks
              aims to provide a constantly updated overview of the current
              state of the art of k-NN algorithms. In the short term, this
              overview allows users to choose the correct k-NN algorithm and
              parameters for their similarity search task; in the longer term,
              algorithm designers will be able to use this overview to test and
              refine automatic parameter tuning. The paper gives an overview of
              the system, evaluates the results of the benchmark, and points
              out directions for future work. Interestingly, very different
              approaches to k-NN search yield comparable quality-performance
              trade-offs. The system is available at http://ann-benchmarks.com.",
  journal  = "Inf. Syst.",
  volume   =  87,
  pages    = "101374",
  month    =  jan,
  year     =  2020,
  keywords = "Benchmarking; Nearest neighbor search; Evaluation"
}

% ANN C++ library
@misc{mount2010-ann,
  author = {Mount, David and Arya, Sunil},
  title = {ANN: A Library for Approximate Nearest Neighbor Searching},
  year = {2010},
  version = {1.1.3},
  howpublished = {\url{https://github.com/jefferislab/RANN}},
  note = {Accessed on 2020-09-24}
}

% k-d tree github repo
@misc{jefferislab2019-l2,
  author = {Mount, David and Arya, Sunil and Kemp, Samuel E. and Jefferis, Gregory and Mülle, Kirill},
  title = {Fast Nearest Neighbour Search (Wraps ANN Library) Using
    L2 Metric},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jefferislab/RANN}},
  note = {Accessed on 2020-09-24}
}

@misc{jefferislab2019-l1,
  author = {Mount, David and Arya, Sunil and Kemp, Samuel E. and Jefferis, Gregory and Mülle, Kirill},
  title = {Fast Nearest Neighbour Search (Wraps ANN Library) Using
    L1 Metric},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jefferislab/RANN/tree/master-L1}},
  note = {Accessed on 2020-09-24}
}


% Annoy github repo
@misc{Bernhardsson2016-tf,
  author = {Spotify},
  title = {Annoy},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/github/open-source-survey}},
  note = {Accessed on 2020-09-24}
}

% Annoy blog
@misc{Bernhardsson2015annoyblog,
  title={Nearest neighbors and vector models – part 2 – algorithms and data structures},
  author={Bernhardsson, Erik},
  url={https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html},
  year={2015},
  note={Accessed on 2020-09-24}
}

% Annoy slides
@misc{Bernhardsson2015-slides,
  author = {Bernhardsson, Erik},
  title = {ANN presentation},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/erikbern/ann-presentation}},
  note = {Accessed on 2020-09-24}
}


% HNSW


@ARTICLE{Malkov2020-jp,
  title    = "Efficient and Robust Approximate Nearest Neighbor Search Using
              Hierarchical Navigable Small World Graphs",
  author   = "Malkov, Yu A and Yashunin, D A",
  abstract = "We present a new approach for the approximate K-nearest neighbor
              search based on navigable small world graphs with controllable
              hierarchy (Hierarchical NSW, HNSW). The proposed solution is
              fully graph-based, without any need for additional search
              structures (typically used at the coarse search stage of the most
              proximity graph techniques). Hierarchical NSW incrementally
              builds a multi-layer structure consisting of a hierarchical set
              of proximity graphs (layers) for nested subsets of the stored
              elements. The maximum layer in which an element is present is
              selected randomly with an exponentially decaying probability
              distribution. This allows producing graphs similar to the
              previously studied Navigable Small World (NSW) structures while
              additionally having the links separated by their characteristic
              distance scales. Starting the search from the upper layer
              together with utilizing the scale separation boosts the
              performance compared to NSW and allows a logarithmic complexity
              scaling. Additional employment of a heuristic for selecting
              proximity graph neighbors significantly increases performance at
              high recall and in case of highly clustered data. Performance
              evaluation has demonstrated that the proposed general metric
              space search index is able to strongly outperform previous
              opensource state-of-the-art vector-only approaches. Similarity of
              the algorithm to the skip list structure allows straightforward
              balanced distributed implementation.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  42,
  number   =  4,
  pages    = "824--836",
  month    =  apr,
  year     =  2020,
  language = "en"
}


@ARTICLE{Zhao2018-wy,
  title         = "Approximate {k-NN} Graph Construction: a Generic Online
                   Approach",
  author        = "Zhao, Wan-Lei",
  abstract      = "Nearest neighbor search and k-nearest neighbor graph
                   construction are two fundamental issues arise from many
                   disciplines such as information retrieval, data-mining and
                   machine learning. Despite continuous efforts have been taken
                   in the last several decades, these two issues remain
                   challenging. They become more and more imminent given the
                   big data emerge in various fields in recent years. In this
                   paper, a simple but effective solution both for k-nearest
                   neighbor search and k-nearest neighbor graph construction is
                   presented. These two issues are addressed jointly in our
                   solution. On one hand, the k-nearest neighbor graph
                   construction is treated as a search task. Each sample along
                   with its k-nearest neighbors are joined into the k-nearest
                   neighbor graph by performing the nearest neighbor search
                   sequentially on the graph under construction. On the other
                   hand, the built k-nearest neighbor graph is used to support
                   k-nearest neighbor search. Since the graph is built online,
                   the dynamic update on the graph, which is not desirable from
                   most of the existing solutions, is supported. This solution
                   is feasible for various distance measures. Its effectiveness
                   both as k-nearest neighbor construction and k-nearest
                   neighbor search approaches is verified across various
                   datasets in different scales, various dimensions and under
                   different metrics.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1804.03032"
}

@article{duong2003plug,
  title={Plug-in bandwidth matrices for bivariate kernel density estimation},
  author={Duong, Tarn and Hazelton, Martin},
  journal={Journal of Nonparametric Statistics},
  volume={15},
  number={1},
  pages={17--30},
  year={2003},
}

@ARTICLE{Belkin2003,
  title   = "Laplacian Eigenmaps for Dimensionality Reduction and Data
             Representation",
  author  = "Belkin, Mikhail and Niyogi, Partha",
  journal = "Neural computation",
  volume  =  15,
  number  =  6,
  pages   = "1373--1396",
  year    =  2003
}



@ARTICLE{Shepard1962b,
  title     = "The analysis of proximities: Multidimensional scaling with an
               unknown distance function. {II}",
  author    = "Shepard, Roger N",
  journal   = "Psychometrika",
  publisher = "Springer-Verlag",
  volume    =  27,
  number    =  3,
  pages     = "219--246",
  year      =  1962
}

@ARTICLE{Shepard1962a,
  title     = "The analysis of proximities: Multidimensional scaling with an
               unknown distance function. {I}",
  author    = "Shepard, Roger N",
  journal   = "Psychometrika",
  publisher = "Springer-Verlag",
  volume    =  27,
  number    =  2,
  pages     = "125--140",
  year      =  1962
}


@ARTICLE{Kruskal1964b,
  title     = "Nonmetric multidimensional scaling: A numerical method",
  author    = "Kruskal, J B",
  journal   = "Psychometrika",
  volume    =  29,
  number    =  2,
  pages     = "115--129",
  year      =  1964
}

@ARTICLE{Kruskal1964a,
  title     = "Multidimensional scaling by optimizing goodness of fit to a
               nonmetric hypothesis",
  author    = "Kruskal, J B",
  journal   = "Psychometrika",
  volume    =  29,
  number    =  1,
  pages     = "1--27",
  year      =  1964
}


@TECHREPORT{Commission_For_Energy_Regulation2011-ub,
  title       = "Electricity smart metering customer behaviour trials findings
                 report",
  author      = "{{Commission For Energy Regulation}}",
  institution = "Dublin: Commission for Energy Regulation",
  year        =  2011,
}

@ARTICLE{Beckel2014-mo,
  title    = "Revealing household characteristics from smart meter data",
  author   = "Beckel, Christian and Sadamori, Leyna and Staake, Thorsten and
              Santini, Silvia",
  journal  = "Energy",
  volume   =  78,
  number   =  0,
  pages    = "397--410",
  year     =  2014,
  keywords = "Data-driven energy efficiency; Domestic electricity consumption;
              Electricity load profiles; Automated customer segmentation;
              Supervised machine learning;\_Smart
              meters;\_IEEE\_TSG;\_MILETS\_KDD2015"
}

@article{Hyndman2018-nq,
  title={Visualizing Big Energy Data: Solutions for This Crucial Component of Data Analysis},
  author={Hyndman, Rob J and Liu, Xueqin Amy and Pinson, Pierre},
  journal={IEEE Power and Energy Magazine},
  volume={16},
  number={3},
  pages={18--25},
  year={2018},
  publisher={IEEE}
}

@Article{HDR96,
  title = {Computing and graphing highest density regions},
  author = {Rob J Hyndman},
  year = {1996},
  number = {2},
  pages = {120--126},
  volume = {50},
  journal = {The American Statistician},
}

@book{Scott2015,
  title={Multivariate density estimation: theory, practice, and visualization},
  author={Scott, David W},
  year={2015},
  edition = {2nd},
  publisher={John Wiley \& Sons}
}





@ARTICLE{Law2006-hi,
  title    = "Incremental nonlinear dimensionality reduction by manifold
              learning",
  author   = "Law, Martin H C and Jain, Anil K",
  abstract = "Understanding the structure of multidimensional patterns,
              especially in unsupervised cases, is of fundamental importance in
              data mining, pattern recognition, and machine learning. Several
              algorithms have been proposed to analyze the structure of
              high-dimensional data based on the notion of manifold learning.
              These algorithms have been used to extract the intrinsic
              characteristics of different types of high-dimensional data by
              performing nonlinear dimensionality reduction. Most of these
              algorithms operate in a ``batch'' mode and cannot be efficiently
              applied when data are collected sequentially. In this paper, we
              describe an incremental version of ISOMAP, one of the key
              manifold learning algorithms. Our experiments on synthetic data
              as well as real world images demonstrate that our modified
              algorithm can maintain an accurate low-dimensional representation
              of the data in an efficient manner.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  28,
  number   =  3,
  pages    = "377--391",
  month    =  mar,
  year     =  2006,
  language = "en"
}

@ARTICLE{Huang2014-vy,
  title    = "A kernel entropy manifold learning approach for financial data
              analysis",
  author   = "Huang, Yan and Kou, Gang",
  abstract = "Identification of intrinsic characteristics and structure of
              high-dimensional data is an important task for financial
              analysis. This paper presents a kernel entropy manifold learning
              algorithm, which employs the information metric to measure the
              relationships between two financial data points and yields a
              reasonable low-dimensional representation of high-dimensional
              financial data. The proposed algorithm can also be used to
              describe the characteristics of a financial system by deriving
              the dynamical properties of the original data space. The
              experiment shows that the proposed algorithm cannot only improve
              the accuracy of financial early warning, but also provide
              objective criteria for explaining and predicting the stock market
              volatility.",
  journal  = "Decis. Support Syst.",
  volume   =  64,
  pages    = "31--42",
  month    =  aug,
  year     =  2014,
  keywords = "Manifold learning; Financial analysis; Low-dimensional embedding;
              Information metric"
}

@ARTICLE{De_Silva_undated-nh,
  title  = "Global versus local methods in nonlinear dimensionality reduction",
  author = "de Silva, Vin and Tenenbaum, Joshua B"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Donoho2003-am,
  title    = "Hessian eigenmaps: locally linear embedding techniques for
              high-dimensional data",
  author   = "Donoho, David L and Grimes, Carrie",
  abstract = "We describe a method for recovering the underlying
              parametrization of scattered data (m(i)) lying on a manifold M
              embedded in high-dimensional Euclidean space. The method,
              Hessian-based locally linear embedding, derives from a conceptual
              framework of local isometry in which the manifold M, viewed as a
              Riemannian submanifold of the ambient Euclidean Space R(n), is
              locally isometric to an open, connected subset $\Theta$ of
              Euclidean space R(d). Because $\Theta$ does not have to be
              convex, this framework is able to handle a significantly wider
              class of situations than the original ISOMAP algorithm. The
              theoretical framework revolves around a quadratic form H(f) =
              $\int$(M)||H(f)(m)||²(F)dm defined on functions f: M--> R. Here
              Hf denotes the Hessian of f, and H(f) averages the Frobenius norm
              of the Hessian over M. To define the Hessian, we use orthogonal
              coordinates on the tangent planes of M. The key observation is
              that, if M truly is locally isometric to an open, connected
              subset of R(d), then H(f) has a (d + 1)-dimensional null space
              consisting of the constant functions and a d-dimensional space of
              functions spanned by the original isometric coordinates. Hence,
              the isometric coordinates can be recovered up to a linear
              isometry. Our method may be viewed as a modification of locally
              linear embedding and our theoretical framework as a modification
              of the Laplacian eigenmaps framework, where we substitute a
              quadratic form based on the Hessian in place of one based on the
              Laplacian.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  100,
  number   =  10,
  pages    = "5591--5596",
  month    =  may,
  year     =  2003,
  language = "en"
}

@INPROCEEDINGS{Dadkhahi2015-ih,
  title     = "Isomap out-of-sample extension for noisy time series data",
  booktitle = "2015 {IEEE} 25th International Workshop on Machine Learning for
               Signal Processing ({MLSP})",
  author    = "Dadkhahi, Hamid and Duarte, Marco F and Marlin, Benjamin",
  abstract  = "Request PDF | Isomap out-of-sample extension for noisy time
               series data | This paper proposes an out-of-sample extension
               framework for a global manifold learning algorithm (Isomap) that
               uses temporal information in... | Find, read and cite all the
               research you need on ResearchGate",
  pages     = "1--6",
  month     =  sep,
  year      =  2015
}

@INPROCEEDINGS{Dornaika2013-ob,
  title     = "{Out-of-Sample} Embedding for Manifold Learning Applied to Face
               Recognition",
  booktitle = "Computer Vision and Pattern Recognition Workshops ({CVPRW)},
               2013 {IEEE} Conference on",
  author    = "Dornaika, F and Raduncanu, B",
  abstract  = "Request PDF | Out-of-Sample Embedding for Manifold Learning
               Applied to Face Recognition | Manifold learning techniques are
               affected by two critical aspects: (i) the design of the
               adjacency graphs, and (ii) the embedding of new test... | Find,
               read and cite all the research you need on ResearchGate",
  pages     = "862--868",
  month     =  jun,
  year      =  2013
}

@ARTICLE{Roweis2002-vm,
  title     = "Global Coordination of Local Linear Models",
  author    = "Roweis, Sam and Saul, Lawrence K and Hinton, Geoffrey E",
  abstract  = "Download Citation | Global Coordination of Local Linear Models |
               High dimensional data that lies on or near a low dimensional
               manifold can be described by a collection of local linear
               models. Such a description,... | Find, read and cite all the
               research you need on ResearchGate",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "Advances in neural information processing systems",
  volume    =  14,
  month     =  feb,
  year      =  2002
}

@ARTICLE{Dadkhahi2017-fs,
  title    = "{Out-of-Sample} Extension for Dimensionality Reduction of Noisy
              Time Series",
  author   = "Dadkhahi, Hamid and Duarte, Marco F and Marlin, Benjamin M",
  abstract = "This paper proposes an out-of-sample extension framework for a
              global manifold learning algorithm (Isomap) that uses temporal
              information in out-of-sample points in order to make the
              embedding more robust to noise and artifacts. Given a set of
              noise-free training data and its embedding, the proposed
              framework extends the embedding for a noisy time series. This is
              achieved by adding a spatio-temporal compactness term to the
              optimization objective of the embedding. To the best of our
              knowledge, this is the first method for out-of-sample extension
              of manifold embeddings that leverages timing information
              available for the extension set. Experimental results demonstrate
              that our out-of-sample extension algorithm renders a more robust
              and accurate embedding of sequentially ordered image data in the
              presence of various noise and artifacts when compared with other
              timing-aware embeddings. Additionally, we show that an
              out-of-sample extension framework based on the proposed algorithm
              outperforms the state of the art in eye-gaze estimation.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  26,
  number   =  11,
  pages    = "5435--5446",
  month    =  nov,
  year     =  2017,
  language = "en"
}

@ARTICLE{Raducanu2014-pa,
  title    = "Embedding new observations via sparse-coding for non-linear
              manifold learning",
  author   = "Raducanu, Bogdan and Dornaika, Fadi",
  abstract = "Non-linear dimensionality reduction techniques are affected by
              two critical aspects: (i) the design of the adjacency graphs, and
              (ii) the embedding of new test data---the out-of-sample problem.
              For the first aspect, the proposed solutions, in general, were
              heuristically driven. For the second aspect, the difficulty
              resides in finding an accurate mapping that transfers unseen data
              samples into an existing manifold. Past works addressing these
              two aspects were heavily parametric in the sense that the optimal
              performance is only achieved for a suitable parameter choice that
              should be known in advance. In this paper, we demonstrate that
              the sparse representation theory not only serves for automatic
              graph construction as shown in recent works, but also represents
              an accurate alternative for out-of-sample embedding. Considering
              for a case study the Laplacian Eigenmaps, we applied our method
              to the face recognition problem. To evaluate the effectiveness of
              the proposed out-of-sample embedding, experiments are conducted
              using the K-nearest neighbor (KNN) and Kernel Support Vector
              Machines (KSVM) classifiers on six public face datasets. The
              experimental results show that the proposed model is able to
              achieve high categorization effectiveness as well as high
              consistency with non-linear embeddings/manifolds obtained in
              batch modes.",
  journal  = "Pattern Recognit.",
  volume   =  47,
  number   =  1,
  pages    = "480--492",
  month    =  jan,
  year     =  2014,
  keywords = "Non-linear manifold learning; Out-of-sample embedding; Sparse
              representation; Face recognition"
}

@ARTICLE{Mendoza_Quispe2016-gk,
  title    = "Extreme learning machine for out-of-sample extension in Laplacian
              eigenmaps",
  author   = "Mendoza Quispe, Arturo and Petitjean, Caroline and Heutte,
              Laurent",
  abstract = "Manifold learning techniques have shown a great potential for
              computer vision problems; however, they do not extend easily to
              points different from the ones on which they were trained
              (out-of-sample). On the other hand, extreme learning machine
              (ELM) is a powerful method that allows to perform nonlinear,
              multivariate regression. This paper discusses the effectiveness
              of ELM for the out-of-sample problem and compares it to the
              state-of-the-art solution : the Nystr{\"o}m extension. Both
              methods are evaluated through the reconstruction of the manifold
              learnt using Laplacian eigenmaps, via experiments on a wide range
              of publicly available image datasets. We show that when reducing
              the data dimension to its intrinsic dimension, the ELM offers a
              better approximation of the embedded coordinates, also with
              reduced computational costs during testing.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  74,
  pages    = "68--73",
  month    =  apr,
  year     =  2016,
  keywords = "Dimensionality reduction; Manifold learning; Laplacian eigenmaps;
              Out-of-sample extension; Extreme learning machine"
}

@ARTICLE{Saul2003-nr,
  title    = "Think Globally, Fit Locally: Unsupervised Learning of Low
              Dimensional Manifolds",
  author   = "Saul, Lawrence K and Roweis, Sam T",
  journal  = "J. Mach. Learn. Res.",
  volume   =  4,
  number   = "Jun",
  pages    = "119--155",
  year     =  2003
}

@ARTICLE{noauthor_undated-gp,

}

@ARTICLE{Belkin2003-kz,
  title    = "Laplacian Eigenmaps for Dimensionality Reduction and Data
              Representation",
  author   = "Belkin, Mikhail and Niyogi, Partha",
  abstract = "Download Citation | Laplacian Eigenmaps for Dimensionality
              Reduction and Data Representation | One of the central problems
              in machine learning and pattern recognition is to develop
              appropriate representations for complex data. We consider... |
              Find, read and cite all the research you need on ResearchGate",
  journal  = "Neural Comput.",
  volume   =  15,
  number   =  6,
  pages    = "1373--1396",
  month    =  jun,
  year     =  2003
}

@INPROCEEDINGS{Yang2010-rm,
  title     = "Local and Global Regressive Mapping for Manifold Learning with
               {Out-of-Sample} Extrapolation",
  booktitle = "Proceedings of the {Twenty-Fourth} {AAAI} Conference on
               Artificial Intelligence, {AAAI} 2010, Atlanta, Georgia, {USA},
               July 11-15, 2010",
  author    = "Yang, Yi and Nie, Feiping and Xiang, Shiming and Zhuang, Yueting
               and Wang, Wenhua",
  abstract  = "PDF | Over the past few years, a large family of manifold
               learning algorithms have been proposed, and applied to various
               applications. While designing... | Find, read and cite all the
               research you need on ResearchGate",
  volume    =  1,
  month     =  jan,
  year      =  2010
}

@ARTICLE{noauthor_undated-ge,

}

@ARTICLE{Chin2008-tm,
  title    = "Out-of-sample extrapolation of learned manifolds",
  author   = "Chin, Tat-Jun and Suter, David",
  abstract = "We investigate the problem of extrapolating the embedding of a
              manifold learned from finite samples to novel out-of-sample data.
              We concentrate on the manifold learning method called Maximum
              Variance Unfolding (MVU) for which the extrapolation problem is
              still largely unsolved. Taking the perspective of MVU learning
              being equivalent to Kernel PCA, our problem reduces to extending
              a kernel matrix generated from an unknown kernel function to
              novel points. Leveraging on previous developments, we propose a
              novel solution which involves approximating the kernel
              eigenfunction using Gaussian basis functions. We also show how
              the width of the Gaussian can be tuned to achieve extrapolation.
              Experimental results which demonstrate the effectiveness of the
              proposed approach are also included.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  30,
  number   =  9,
  pages    = "1547--1556",
  month    =  sep,
  year     =  2008,
  language = "en"
}

@INCOLLECTION{Bengio2004-ei,
  title     = "{Out-of-Sample} Extensions for {LLE}, Isomap, {MDS}, Eigenmaps,
               and Spectral Clustering",
  booktitle = "Advances in Neural Information Processing Systems 16",
  author    = "Bengio, Yoshua and Paiement, Jean-Fran{\c c}cois and Vincent,
               Pascal and Delalleau, Olivier and Roux, Nicolas L and Ouimet,
               Marie",
  editor    = "Thrun, S and Saul, L K and Sch{\"o}lkopf, B",
  publisher = "MIT Press",
  pages     = "177--184",
  year      =  2004
}

@ARTICLE{Bengio2004-yq,
  title    = "Learning eigenfunctions links spectral embedding and kernel {PCA}",
  author   = "Bengio, Yoshua and Delalleau, Olivier and Le Roux, Nicolas and
              Paiement, Jean-Fran{\c c}ois and Vincent, Pascal and Ouimet,
              Marie",
  abstract = "In this letter, we show a direct relation between spectral
              embedding methods and kernel principal components analysis and
              how both are special cases of a more general learning problem:
              learning the principal eigenfunctions of an operator defined from
              a kernel and the unknown data-generating density. Whereas
              spectral embedding methods provided only coordinates for the
              training points, the analysis justifies a simple extension to
              out-of-sample examples (the Nystr{\"o}m formula) for
              multidimensional scaling (MDS), spectral clustering, Laplacian
              eigenmaps, locally linear embedding (LLE), and Isomap. The
              analysis provides, for all such spectral embedding methods, the
              definition of a loss function, whose empirical average is
              minimized by the traditional algorithms. The asymptotic expected
              value of that loss defines a generalization performance and
              clarifies what these algorithms are trying to learn. Experiments
              with LLE, Isomap, spectral clustering, and MDS show that this
              out-of-sample embedding formula generalizes well, with a level of
              error comparable to the effect of small perturbations of the
              training set on the embedding.",
  journal  = "Neural Comput.",
  volume   =  16,
  number   =  10,
  pages    = "2197--2219",
  month    =  oct,
  year     =  2004,
  language = "en"
}

@ARTICLE{Webber2018-zt,
  title     = "Integration of Tumor Genomic Data with Cell Lines Using
               Multi-dimensional Network Modules Improves Cancer
               Pharmacogenomics",
  author    = "Webber, James T and Kaushik, Swati and Bandyopadhyay, Sourav",
  abstract  = "Leveraging insights from genomic studies of patient tumors is
               limited by the discordance between these tumors and the cell
               line models used for functional studies. We integrate omics
               datasets using functional networks to identify gene modules
               reflecting variation between tumors and show that the structure
               of these modules can be evaluated in cell lines to discover
               clinically relevant biomarkers of therapeutic responses. Applied
               to breast cancer, we identify 219 gene modules that capture
               recurrent alterations and subtype patients and quantitate
               various cell types within the tumor microenvironment. Comparison
               of modules between tumors and cell lines reveals that many
               modules composed primarily of gene expression and methylation
               are poorly preserved. In contrast, preserved modules are highly
               predictive of drug responses in a manner that is robust and
               clinically relevant. This work addresses a fundamental challenge
               in pharmacogenomics that can only be overcome by the joint
               analysis of patient and cell line data.",
  journal   = "Cell Syst",
  publisher = "Elsevier",
  volume    =  7,
  number    =  5,
  pages     = "526--536.e6",
  month     =  nov,
  year      =  2018,
  keywords  = "biomarkers; breast cancer; data integration; networks;
               pharmacogenomics; therapeutics",
  language  = "en"
}

@INCOLLECTION{Law2004-qe,
  title     = "Nonlinear Manifold Learning For Data Stream",
  booktitle = "Proceedings of the 2004 {SIAM} International Conference on Data
               Mining",
  author    = "Law, Martin H C and Zhang, Nan and Jain, Anil K",
  abstract  = "Abstract There has been a renewed interest in understanding the
               structure of high dimensional data set based on manifold
               learning. Examples include ISOMAP [25], LLE [20] and Laplacian
               Eigenmap [2] algorithms. Most of these algorithms operate in a
               ?batch? mode and cannot be applied efficiently for a data
               stream. We propose an incremental version of ISOMAP. Our
               experiments not only demonstrate the accuracy and efficiency of
               the proposed algorithm, but also reveal interesting behavior of
               the ISOMAP as the size of available data increases.",
  publisher = "Society for Industrial and Applied Mathematics",
  pages     = "33--44",
  series    = "Proceedings",
  month     =  apr,
  year      =  2004
}

@ARTICLE{Hyndman1996-lk,
  title     = "Computing and Graphing Highest Density Regions",
  author    = "Hyndman, Rob J",
  abstract  = "[Many statistical methods involve summarizing a probability
               distribution by a region of the sample space covering a
               specified probability. One method of selecting such a region is
               to require it to contain points of relatively high density.
               Highest density regions are particularly useful for displaying
               multimodal distributions and, in such cases, may consist of
               several disjoint subsets-one for each local mode. In this paper,
               I propose a simple method for computing a highest density region
               from any given (possibly multivariate) density f(x) that is
               bounded and continuous in x. Several examples of the use of
               highest density regions in statistical graphics are also given.
               A new form of boxplot is proposed based on highest density
               regions; versions in one and two dimensions are given. Highest
               density regions in higher dimensions are also discussed and
               plotted.]",
  journal   = "Am. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  50,
  number    =  2,
  pages     = "120--126",
  year      =  1996
}

@INCOLLECTION{De_Silva2003-mw,
  title     = "Unsupervised Learning of Curved Manifolds",
  booktitle = "Nonlinear Estimation and Classification",
  author    = "de Silva, Vin and Tenenbaum, Joshua B",
  editor    = "Denison, David D and Hansen, Mark H and Holmes, Christopher C
               and Mallick, Bani and Yu, Bin",
  abstract  = "We describe a variant of the Isomap manifold learning algorithm
               [1], called `C-Isomap'. Isomap was designed to learn non-linear
               mappings which are isometric embeddings of a flat, convex data
               set. C-Isomap is designed to recover mappings in the larger
               class of conformal embeddings, provided that the original
               sampling density is reasonably uniform. We compare the
               performance of both versions of Isomap and other algorithms for
               manifold learning (MDS, LLE, GTM) on a range of data sets.",
  publisher = "Springer New York",
  pages     = "453--465",
  year      =  2003,
  address   = "New York, NY"
}

@ARTICLE{Tan2018-rh,
  title    = "Sparse generalized eigenvalue problem: optimal statistical rates
              via truncated Rayleigh flow",
  author   = "Tan, Kean Ming and Wang, Zhaoran and Liu, Han and Zhang, Tong",
  abstract = "Summary The sparse generalized eigenvalue problem (GEP) plays a
              pivotal role in a large family of high dimensional statistical
              models, including sparse Fisher's discriminant analysis,
              canonical correlation analysis and sufficient dimension
              reduction. The sparse GEP involves solving a non-convex
              optimization problem. Most existing methods and theory in the
              context of specific statistical models that are special cases of
              the sparse GEP require restrictive structural assumptions on the
              input matrices. We propose a two-stage computational framework to
              solve the sparse GEP. At the first stage, we solve a convex
              relaxation of the sparse GEP. Taking the solution as an initial
              value, we then exploit a non-convex optimization perspective and
              propose the truncated Rayleigh flow method (which we call
              ?rifle?) to estimate the leading generalized eigenvector. We show
              that rifle converges linearly to a solution with the optimal
              statistical rate of convergence. Theoretically, our method
              significantly improves on the existing literature by eliminating
              structural assumptions on the input matrices. To achieve this,
              our analysis involves two key ingredients: a new analysis of the
              gradient-based method on non-convex objective functions, and a
              fine-grained characterization of the evolution of sparsity
              patterns along the solution path. Thorough numerical studies are
              provided to validate the theoretical results.",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  80,
  number   =  5,
  pages    = "1057--1086",
  month    =  nov,
  year     =  2018
}

@ARTICLE{Bartenhagen2019-dq,
  title  = "{RDRToolbox}: A package for nonlinear dimension reduction with
            Isomap and {LLE}",
  author = "Bartenhagen, C",
  year   =  2019
}

@ARTICLE{Mardia_undated-ve,
  title   = "Multivariate analysis. 1979",
  author  = "Mardia, K V and Kent, J T and Bibby, J M",
  journal = "Probability and mathematical statistics. Academic Press Inc"
}

@ARTICLE{Bronstein2006-ly,
  title    = "Generalized multidimensional scaling: a framework for
              isometry-invariant partial surface matching",
  author   = "Bronstein, Alexander M and Bronstein, Michael M and Kimmel, Ron",
  abstract = "An efficient algorithm for isometry-invariant matching of
              surfaces is presented. The key idea is computing the
              minimum-distortion mapping between two surfaces. For this
              purpose, we introduce the generalized multidimensional scaling, a
              computationally efficient continuous optimization algorithm for
              finding the least distortion embedding of one surface into
              another. The generalized multidimensional scaling algorithm
              allows for both full and partial surface matching. As an example,
              it is applied to the problem of expression-invariant
              three-dimensional face recognition.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  103,
  number   =  5,
  pages    = "1168--1172",
  month    =  jan,
  year     =  2006,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Nguyen2019-hm,
  title     = "Ten quick tips for effective dimensionality reduction",
  author    = "Nguyen, Lan Huong and Holmes, Susan",
  abstract  = "Dimensionality reduction (DR) is frequently applied during the
               analysis of high-dimensional data. Both a means of denoising and
               simplification, it can be beneficial for the majority of modern
               biological datasets, in which it's not uncommon to have hundreds
               or even millions of simultaneous measurements collected for a
               single sample. Because of ``the curse of dimensionality,'' many
               statistical methods lack power when applied to high-dimensional
               data. Even if the number of collected data points is large, they
               remain sparsely submerged in …",
  journal   = "PLoS Comput. Biol.",
  publisher = "journals.plos.org",
  volume    =  15,
  number    =  6,
  pages     = "e1006907",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{Renyi1961-xe,
  title       = "On measures of entropy and information",
  booktitle   = "Proceedings of the Fourth Berkeley Symposium on Mathematical
                 Statistics and Probability, Volume 1: Contributions to the
                 Theory of Statistics",
  author      = "R{\'e}nyi, Alfr{\'e}d and {Others}",
  abstract    = "1. Characterization of Shannon's measure of entropy Let
                 d'=(pI, P2,-, pn,) be a finite discreteprobability
                 distribution, that is, suppose pk \_ O (k= 1, 2,*, n) and tl
                 Pk= 1. The amount of un-certainty of the distribution (P, that
                 is, the amount of uncertainty concerning the outcome of an
                 experiment, the possible results of which have the
                 probabilities PI, P2,*** p, n, is called the entropy of the
                 distribution (P and is usually measured by the quantity H
                 [(P]= H (p1, P2,** pn), introduced by Shannon [1] and defined
                 by n 1 (1.1)",
  publisher   = "researchgate.net",
  institution = "The Regents of the University of California",
  year        =  1961
}

@BOOK{Kullback1997-fw,
  title     = "Information Theory and Statistics",
  author    = "Kullback, Solomon",
  abstract  = "Highly useful text studies logarithmic measures of information
               and their application to testing statistical hypotheses.
               Includes numerous worked examples and problems. References.
               Glossary. Appendix. 1968 2nd, revised edition.",
  publisher = "Courier Corporation",
  month     =  jul,
  year      =  1997,
  language  = "en"
}

@ARTICLE{Cha2007-ra,
  title     = "Comprehensive survey on distance/similarity measures between
               probability density functions",
  author    = "Cha, S H",
  abstract  = "Distance or similarity measures are essential to solve many
               pattern recognition problems such as classification, clustering,
               and retrieval problems. Various distance/similarity measures
               that are applicable to compare two probability density
               functions, pdf in short, are reviewed and categorized in both
               syntactic and semantic relationships. A correlation coefficient
               and a hierarchical clustering technique are adopted to reveal
               similarities among numerous distance/similarity measures.",
  journal   = "Cityscape",
  publisher = "Citeseer",
  year      =  2007
}

@ARTICLE{Kapur1983-nt,
  title     = "A Comparative Assessment of Various Measures of Entropy",
  author    = "Kapur, J N",
  abstract  = "A large number of measures of entropy have been proposed by
               Hartley [21], Shannon [46], Renyi [43], Havrada and Charvat
               [22], Aczel and Daroczy [5], Kapur [25, 26,27], Rathie [42],
               Behara and Chawla [11], Sharma and Taneja [47] and others. These
               do not measure the same entity. Moreover the definitions of the
               various measures have been motivated by quite different
               considerations. The use of the same word ?entropy? for so many
               Intrinsically different entities is confusing and unfortunate.
               The present paper attempts to explain each of the entropies in
               its proper perspective by making a comparative assessment of the
               various measures proposed.",
  journal   = "J. Inf. Optimiz. Sci.",
  publisher = "Taylor \& Francis",
  volume    =  4,
  number    =  3,
  pages     = "207--232",
  month     =  jan,
  year      =  1983
}

@INPROCEEDINGS{Sutherland2016-pp,
  title     = "{Linear-Time} Learning on Distributions with Approximate Kernel
               Embeddings",
  booktitle = "Thirtieth {AAAI} Conference on Artificial Intelligence",
  author    = "Sutherland, Dougal J and Oliva, Junier B and P{\'o}czos,
               Barnab{\'a}s and Schneider, Jeff",
  abstract  = "Many interesting machine learning problems are best posed by
               considering instances that are distributions, or sample sets
               drawn from distributions. Most previous work devoted to machine
               learning tasks with distributional inputs has done so through
               pairwise kernel evaluations between pdfs (or sample sets). While
               such an approach is fine for smaller datasets, the computation
               of an N $\times$ N Gram matrix is prohibitive in large datasets.
               Recent scalable estimators that work over pdfs have done so only
               with kernels that use Euclidean metrics, like the L 2 distance.
               However, there are a myriad of other useful metrics available,
               such as total variation, Hellinger distance, and the
               Jensen-Shannon divergence. This work develops the first random
               features for pdfs whose dot product approximates kernels using
               these non-Euclidean metrics. These random features allow
               estimators to scale to large datasets by working in a primal
               space, without computing large Gram matrices. We provide an
               analysis of the approximation error in using our proposed random
               features, and show empirically the quality of our approximation
               both in estimating a Gram matrix and in solving learning tasks
               in real-world and synthetic data.",
  month     =  mar,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Devroye2018-ii,
  title         = "The total variation distance between high-dimensional
                   Gaussians",
  author        = "Devroye, Luc and Mehrabian, Abbas and Reddad, Tommy",
  abstract      = "We prove a lower bound and an upper bound for the total
                   variation distance between two high-dimensional Gaussians,
                   which are within a constant factor of one another.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1810.08693"
}

@ARTICLE{Lin1991-ge,
  title    = "Divergence measures based on the Shannon entropy",
  author   = "Lin, J",
  abstract = "A novel class of information-theoretic divergence measures based
              on the Shannon entropy is introduced. Unlike the well-known
              Kullback divergences, the new measures do not require the
              condition of absolute continuity to be satisfied by the
              probability distributions involved. More importantly, their close
              relationship with the variational distance and the probability of
              misclassification error are established in terms of bounds. These
              bounds are crucial in many applications of divergence measures.
              The measures are also well characterized by the properties of
              nonnegativity, finiteness, semiboundedness, and boundedness.>",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  37,
  number   =  1,
  pages    = "145--151",
  month    =  jan,
  year     =  1991,
  keywords = "entropy;information theory;information theory;Shannon
              entropy;divergence measures;variational distance;probability of
              misclassification
              error;nonnegativity;finiteness;semiboundedness;boundedness;Entropy;Probability
              distribution;Upper bound;Pattern analysis;Signal analysis;Signal
              processing;Pattern recognition;Taxonomy;Genetics;Computer science"
}

@ARTICLE{Topsoe_undated-hg,
  title  = "Some inequalities for information divergence and related measures
            of discrimination",
  author = "Tops{\o}e, Flemming"
}

@ARTICLE{Fuglede_undated-tl,
  title  = "{Jensen-Shannon} Divergence and Hilbert space embedding",
  author = "Fuglede, Bent and Topse, Flemming"
}

@ARTICLE{Lee2000-tr,
  title         = "Measures of Distributional Similarity",
  author        = "Lee, Lillian",
  abstract      = "We study distributional similarity measures for the purpose
                   of improving probability estimation for unseen
                   cooccurrences. Our contributions are three-fold: an
                   empirical comparison of a broad range of measures; a
                   classification of similarity functions based on the
                   information that they incorporate; and the introduction of a
                   novel function that is superior at evaluating potential
                   proxy distributions.",
  month         =  jan,
  year          =  2000,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "cs/0001012"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Topsoe2003-za,
  title     = "Jenson-shannon divergence and norm-based measures of
               discrimination and variation",
  author    = "Tops{\o}e, Flemming",
  abstract  = "… are symmetric in a natural sense and smoothing is achieved by
               consideration of the midpoint distribution M. The specific
               Jensen - Shannon divergence may be expressed in terms of 5 Page
               6. the midpoint probabilities mn = 1 2 (pn +qn) and the
               probability distributions Tn $\in$ M1 …",
  journal   = "preprint",
  publisher = "math.ku.dk",
  year      =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cha2007-gp,
  title     = "Comprehensive survey on distance/similarity measures between
               probability density functions",
  author    = "Cha, Sung-Hyuk",
  abstract  = "… considered and other types of histogram are abstained. When
               each bin is divided by n, the probability density function which
               represents a probability distribution is produced. A pdf for a
               corresponding histogram is produced …",
  journal   = "Cityscape",
  publisher = "Citeseer",
  volume    =  1,
  number    =  2,
  pages     = "1",
  year      =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Hinton2003-pc,
  title     = "Stochastic Neighbor Embedding",
  booktitle = "Advances in Neural Information Processing Systems 15",
  author    = "Hinton, Geoffrey E and Roweis, Sam T",
  editor    = "Becker, S and Thrun, S and Obermayer, K",
  abstract  = "We describe a probabilistic approach to the task of placing
               objects, described by high- dimensional vectors or by pairwise
               dissimilarities, in a low-dimensional space in a way that
               preserves neighbor identities. A Gaussian is centered on each
               object in the high …",
  publisher = "MIT Press",
  pages     = "857--864",
  year      =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_Der_Maaten2014-in,
  title     = "Accelerating {t-SNE} using tree-based algorithms",
  author    = "Van Der Maaten, Laurens",
  abstract  = "The paper investigates the acceleration of t - SNE ---an
               embedding technique that is commonly used for the visualization
               of high-dimensional data in scatter plots---using two treebased
               algorithms. In particular, the paper develops variants of the
               Barnes-Hut algorithm …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR. org",
  volume    =  15,
  number    =  1,
  pages     = "3221--3245",
  year      =  2014
}

@ARTICLE{Paul_Brooks_undated-mh,
  title  = "pcaL1: An Implementation in {R} of Three Methods for {L1-Norm}
            Principal Component Analysis",
  author = "Paul Brooks, J and Jot, Sapan"
}

@ARTICLE{Lee_undated-gr,
  title  = "Quality assessment of nonlinear dimensionality reduction based on
            {K} -ary neighborhoods",
  author = "Lee, John A and Verleysen, Michel"
}

@TECHREPORT{Mount1998-kc,
  title       = "{ANN} programming manual",
  author      = "Mount, David M",
  institution = "Technical report, Dept. of Computer Science, U. of Maryland",
  year        =  1998
}

@ARTICLE{Fowlkes2004-sz,
  title    = "Spectral grouping using the Nystr{\"o}m method",
  author   = "Fowlkes, Charless and Belongie, Serge and Chung, Fan and Malik,
              Jitendra",
  abstract = "Spectral graph theoretic methods have recently shown great
              promise for the problem of image segmentation. However, due to
              the computational demands of these approaches, applications to
              large problems such as spatiotemporal data and high resolution
              imagery have been slow to appear. The contribution of this paper
              is a method that substantially reduces the computational
              requirements of grouping algorithms based on spectral
              partitioning making it feasible to apply them to very large
              grouping problems. Our approach is based on a technique for the
              numerical solution of eigenfunction problems known as the
              Nystr{\"o}m method. This method allows one to extrapolate the
              complete grouping solution using only a small number of samples.
              In doing so, we leverage the fact that there are far fewer
              coherent groups in a scene than pixels.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  26,
  number   =  2,
  pages    = "214--225",
  month    =  feb,
  year     =  2004,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Baraniuk2009-ob,
  title     = "Random Projections of Smooth Manifolds",
  author    = "Baraniuk, Richard G and Wakin, Michael B",
  abstract  = "We propose a new approach for nonadaptive dimensionality
               reduction of manifold-modeled data, demonstrating that a small
               number of random linear projections can preserve key information
               about a manifold-modeled signal. We center our analysis on the
               effect of a random linear projection operator
               $\Phi$:ℝN$\rightarrow$ℝM, M<N, on a smooth well-conditioned
               K-dimensional submanifold ℳ$\subset$ℝN. As our main theoretical
               contribution, we establish a sufficient number M of random
               projections to guarantee that, with high probability, all
               pairwise Euclidean and geodesic distances between points on ℳ
               are well preserved under the mapping $\Phi$.",
  journal   = "Found. Comut. Math.",
  publisher = "Springer",
  volume    =  9,
  number    =  1,
  pages     = "51--77",
  month     =  feb,
  year      =  2009
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Liu2005-fh,
  title     = "An Investigation of Practical Approximate Nearest Neighbor
               Algorithms",
  booktitle = "Advances in Neural Information Processing Systems 17",
  author    = "Liu, Ting and Moore, Andrew W and Yang, Ke and Gray, Alexander G",
  editor    = "Saul, L K and Weiss, Y and Bottou, L",
  abstract  = "This paper concerns approximate nearest neighbor searching
               algorithms, which have become increasingly important, especially
               in high dimensional perception areas such as computer vision,
               with dozens of publications in recent years. Much of this
               enthusiasm is due to a successful new approximate nearest
               neighbor approach called Locality Sensitive Hashing (LSH). In
               this paper we ask the question: can earlier spatial data
               structure approaches to exact nearest neighbor, such as metric
               trees, be altered to provide …",
  publisher = "MIT Press",
  pages     = "825--832",
  year      =  2005
}

@ARTICLE{Dimitris2001-ub,
  title     = "Database-friendly random projections",
  author    = "{Dimitris} and {Achlioptas}",
  abstract  = "Database-friendly random projections Dimitris Achlioptas.
               Proceedings of the twentieth ACM SIGMOD-SIGACTSIGART symposium
               on Principles of database systems,PODS'01 New York, NY,USA,
               274-281, 2001",
  journal   = "Proceedings of the twentieth ACM SIGMOD-SIGACTSIGART symposium
               on Principles of database systems,PODS'01 New York, NY,USA",
  publisher = "ACM",
  pages     = "274--281",
  year      =  2001
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Uhlmann1991-rz,
  title     = "Metric trees",
  author    = "Uhlmann, Jeffrey K",
  abstract  = "A new multidimensional search structure is described that is
               able to exploit metric information to efficiently satisfy B
               large class of proximity queries. Numerous problems in
               computational geometry require the efficient identification of
               elements from a finite set of …",
  journal   = "Appl. Math. Lett.",
  publisher = "Pergamon",
  volume    =  4,
  number    =  5,
  pages     = "61--62",
  year      =  1991
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kumar2012-jh,
  title     = "Sampling Methods for the Nystr{\"o}m Method",
  author    = "Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet",
  abstract  = "… SMGA was proposed as a sampling scheme to be used in
               conjunction with the Nystr{\"o}m method , ICL generates a … It
               requires a full pass through K in each iteration and is thus
               inefficient for large K. In … l/n\% Data Set Uniform ICL SMGA
               Adapt-Part K-means Adapt- Full PIE-2.7K 39.7 …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  13,
  number    = "Apr",
  pages     = "981--1006",
  year      =  2012
}

@ARTICLE{Zhang2010-ww,
  title     = "Clustered Nystr{\"o}m method for large scale manifold learning
               and dimension reduction",
  author    = "Zhang, Kai and Kwok, James T",
  abstract  = "Kernel (or similarity) matrix plays a key role in many machine
               learning algorithms such as kernel methods, manifold learning,
               and dimension reduction. However, the cost of storing and
               manipulating the complete kernel matrix makes it infeasible for
               large problems. The Nystr{\"o}m method is a popular
               sampling-based low-rank approximation scheme for reducing the
               computational burdens in handling large kernel matrices. In this
               paper, we analyze how the approximating quality of the
               Nystr{\"o}m method depends on the choice of landmark points, and
               in particular the encoding powers of the landmark points in
               summarizing the data. Our (non-probabilistic) error analysis
               justifies a ``clustered Nystr{\"o}m method'' that uses the
               k-means clustering centers as landmark points. Our algorithm can
               be applied to scale up a wide variety of algorithms that depend
               on the eigenvalue decomposition of kernel matrix (or its
               variant), such as kernel principal component analysis, Laplacian
               eigenmap, spectral clustering, as well as those involving kernel
               matrix inverse such as least-squares support vector machine and
               Gaussian process regression. Extensive experiments demonstrate
               the competitive performance of our algorithm in both accuracy
               and efficiency.",
  journal   = "IEEE Trans. Neural Netw.",
  publisher = "ieeexplore.ieee.org",
  volume    =  21,
  number    =  10,
  pages     = "1576--1587",
  month     =  oct,
  year      =  2010,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Talwalkar2013-kc,
  title     = "Large-scale {SVD} and manifold learning",
  author    = "Talwalkar, Ameet and Kumar, Sanjiv and Mohri, Mehryar and
               Rowley, Henry",
  abstract  = "This paper examines the efficacy of sampling-based low-rank
               approximation techniques when applied to large dense kernel
               matrices. We analyze two common approximate singular value
               decomposition techniques, namely the Nystr{\"o}m and Column
               sampling …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR. org",
  volume    =  14,
  number    =  1,
  pages     = "3129--3152",
  year      =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Hegde2008-tc,
  title     = "Random Projections for Manifold Learning",
  booktitle = "Advances in Neural Information Processing Systems 20",
  author    = "Hegde, Chinmay and Wakin, Michael and Baraniuk, Richard",
  editor    = "Platt, J C and Koller, D and Singer, Y and Roweis, S T",
  abstract  = "We propose a novel method for \{\textbackslashem linear\}
               dimensionality reduction of manifold modeled data. First, we
               show that with a small number $ M $ of \{\textbackslashem random
               projections\} of sample points in $\reals^ N $ belonging to an
               unknown $ K $-dimensional Euclidean manifold , the …",
  publisher = "Curran Associates, Inc.",
  pages     = "641--648",
  year      =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Ma2011-tx,
  title     = "Manifold learning theory and applications",
  author    = "Ma, Yunqian and Fu, Yun",
  abstract  = "Scientists and engineers working with large volumes of
               high-dimensional data often face the problem of dimensionality
               reduction: finding meaningful low-dimensional structures hidden
               in their high-dimensional observations. Manifold learning, as an
               important mathematical tool …",
  publisher = "CRC press",
  year      =  2011
}

@ARTICLE{Roweis2000-ni,
  title     = "Nonlinear dimensionality reduction by locally linear embedding",
  author    = "Roweis, S T and Saul, L K",
  abstract  = "Many areas of science depend on exploratory data analysis and
               visualization. The need to analyze large amounts of multivariate
               data raises the fundamental problem of dimensionality reduction:
               how to discover compact representations of high-dimensional
               data. Here, we introduce locally linear embedding (LLE), an
               unsupervised learning algorithm that computes low-dimensional,
               neighborhood-preserving embeddings of high-dimensional inputs.
               Unlike clustering methods for local dimensionality reduction,
               LLE maps its inputs into a single global coordinate system of
               lower dimensionality, and its optimizations do not involve local
               minima. By exploiting the local symmetries of linear
               reconstructions, LLE is able to learn the global structure of
               nonlinear manifolds, such as those generated by images of faces
               or documents of text.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  290,
  number    =  5500,
  pages     = "2323--2326",
  month     =  dec,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Tenenbaum2000-fr,
  title     = "A global geometric framework for nonlinear dimensionality
               reduction",
  author    = "Tenenbaum, J B and de Silva, V and Langford, J C",
  abstract  = "Scientists working with large volumes of high-dimensional data,
               such as global climate patterns, stellar spectra, or human gene
               distributions, regularly confront the problem of dimensionality
               reduction: finding meaningful low-dimensional structures hidden
               in their high-dimensional observations. The human brain
               confronts the same problem in everyday perception, extracting
               from its high-dimensional sensory inputs-30,000 auditory nerve
               fibers or 10(6) optic nerve fibers-a manageably small number of
               perceptually relevant features. Here we describe an approach to
               solving dimensionality reduction problems that uses easily
               measured local metric information to learn the underlying global
               geometry of a data set. Unlike classical techniques such as
               principal component analysis (PCA) and multidimensional scaling
               (MDS), our approach is capable of discovering the nonlinear
               degrees of freedom that underlie complex natural observations,
               such as human handwriting or images of a face under different
               viewing conditions. In contrast to previous algorithms for
               nonlinear dimensionality reduction, ours efficiently computes a
               globally optimal solution, and, for an important class of data
               manifolds, is guaranteed to converge asymptotically to the true
               structure.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  290,
  number    =  5500,
  pages     = "2319--2323",
  month     =  dec,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Izenman2012-mx,
  title     = "Introduction to manifold learning",
  author    = "Izenman, Alan Julian",
  abstract  = "Abstract A popular research area today in statistics and machine
               learning is that of manifold learning, which is related to the
               algorithmic techniques of dimensionality reduction. Manifold
               learning can be divided into linear and nonlinear methods.
               Linear methods, which have long been part of the statistician's
               toolbox for analyzing multivariate data, include principal
               component analysis (PCA) and multidimensional scaling (MDS).
               Recently, there has been a flurry of research activity on
               nonlinear manifold learning, which includes Isomap, local linear
               embedding, Laplacian eigenmaps, Hessian eigenmaps, and diffusion
               maps. Some of these techniques are nonlinear generalizations of
               the linear methods. The algorithmic process of most of these
               techniques consists of three steps: a nearest-neighbor search, a
               definition of distances or affinities between points (a key
               ingredient for the success of these methods), and an
               eigenproblem for embedding high-dimensional points into a lower
               dimensional space. This article gives us a brief survey of these
               new methods and indicates their strengths and weaknesses. WIREs
               Comput Stat 2012 doi: 10.1002/wics.1222 This article is
               categorized under: Statistical and Graphical Methods of Data
               Analysis > Dimension Reduction Statistical Learning and
               Exploratory Methods of the Data Sciences > Manifold Learning
               Statistical and Graphical Methods of Data Analysis >
               Multivariate Analysis",
  journal   = "WIREs Comp Stat",
  publisher = "Wiley Online Library",
  volume    =  4,
  number    =  5,
  pages     = "439--446",
  month     =  sep,
  year      =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Talwalkar2008-fl,
  title     = "Large-scale manifold learning",
  author    = "Talwalkar, A and Kumar, S and Rowley, H",
  abstract  = "This paper examines the problem of extracting low-dimensional
               manifold structure given millions of high-dimensional face
               images. Specifically, we address the computational challenges of
               nonlinear dimensionality reduction via Isomap and Laplacian
               Eigenmaps …",
  journal   = "2008 IEEE Conference on",
  publisher = "ieeexplore.ieee.org",
  year      =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cayton2005-dp,
  title     = "Algorithms for manifold learning",
  author    = "Cayton, Lawrence",
  abstract  = "Manifold learning is a popular recent approach to nonlinear
               dimensionality reduction. Algorithms for this task are based on
               the idea that the dimensionality of many data sets is only
               artificially high; though each data point consists of perhaps
               thousands of features, it …",
  journal   = "Univ. of California at San Diego Tech. Rep",
  publisher = "cseweb.ucsd.edu",
  volume    =  12,
  number    = "1-17",
  pages     = "1",
  year      =  2005
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Bregler1995-fy,
  title       = "Nonlinear manifold learning for visual speech recognition",
  booktitle   = "Proceedings of {IEEE} International Conference on Computer
                 Vision",
  author      = "Bregler, Christoph and Omohundro, Stephen M",
  abstract    = "… We use this technique to learn the ``space of lips'' in a
                 visual speech recognition task … The first section of this
                 paper describes the mani - fold representation and learning
                 algorithm. Next we describe the use of learned manifolds for
                 interpolation …",
  publisher   = "ieeexplore.ieee.org",
  pages       = "494--499",
  institution = "IEEE",
  year        =  1995
}

@ARTICLE{Kohonen2013-eh,
  title     = "Essentials of the self-organizing map",
  author    = "Kohonen, Teuvo",
  abstract  = "The self-organizing map (SOM) is an automatic data-analysis
               method. It is widely applied to clustering problems and data
               exploration in industry, finance, natural sciences, and
               linguistics. The most extensive applications, exemplified in
               this paper, can be found in the management of massive textual
               databases and in bioinformatics. The SOM is related to the
               classical vector quantization (VQ), which is used extensively in
               digital signal processing and transmission. Like in VQ, the SOM
               represents a distribution of input data items using a finite set
               of models. In the SOM, however, these models are automatically
               associated with the nodes of a regular (usually two-dimensional)
               grid in an orderly fashion such that more similar models become
               automatically associated with nodes that are adjacent in the
               grid, whereas less similar models are situated farther away from
               each other in the grid. This organization, a kind of similarity
               diagram of the models, makes it possible to obtain an insight
               into the topographic relationships of data, especially of
               high-dimensional data items. If the data items belong to certain
               predetermined classes, the models (and the nodes) can be
               calibrated according to these classes. An unknown input item is
               then classified according to that node, the model of which is
               most similar with it in some metric used in the construction of
               the SOM. A new finding introduced in this paper is that an input
               item can even more accurately be represented by a linear mixture
               of a few best-matching models. This becomes possible by a
               least-squares fitting procedure where the coefficients in the
               linear mixture of models are constrained to nonnegative values.",
  journal   = "Neural Netw.",
  publisher = "Elsevier",
  volume    =  37,
  pages     = "52--65",
  month     =  jan,
  year      =  2013,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kohonen1996-to,
  title     = "Som pak: The self-organizing map program package",
  author    = "Kohonen, Teuvo and Hynninen, Jussi and Kangas, Jari and
               Laaksonen, Jorma",
  abstract  = "The Self - Organizing Map (SOM) represents the result of a
               vector quantization algorithm that places a number of reference
               or codebook vectors into a high-dimensional input data space to
               approximate to its data sets in an ordered fashion. The SOM PAK
               program package …",
  journal   = "Report A31, Helsinki University of Technology, Laboratory of
               Computer and Information Science",
  publisher = "hackbbs.org",
  volume    =  1,
  number    = "1-27",
  pages     = "39--40",
  year      =  1996
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kohonen1997-rv,
  title       = "Exploration of very large databases by self-organizing maps",
  booktitle   = "Proceedings of International Conference on Neural Networks
                 ({ICNN'97})",
  author      = "Kohonen, Teuvo",
  abstract    = "This paper describes a data organization system and genuine
                 content-addressable memory called the WEBSOM. It is a
                 two-layer self - organizing map (SOM) architecture where
                 documents become mapped as points on the upper map , in a
                 geometric order that describes …",
  publisher   = "ieeexplore.ieee.org",
  volume      =  1,
  pages       = "PL1--PL6",
  institution = "IEEE",
  year        =  1997
}

@BOOK{Kohonen2012-gq,
  title     = "{Self-Organizing} Maps",
  author    = "Kohonen, Teuvo",
  abstract  = "Since the second edition of this book came out in early 1997,
               the number of scientific papers published on the Self-Organizing
               Map (SOM) has increased from about 1500 to some 4000. Also, two
               special workshops dedicated to the SOM have been organized, not
               to mention numerous SOM sessions in neural network conferences.
               In view of this growing interest it was felt desirable to make
               extensive revisions to this book. They are of the following
               nature. Statistical pattern analysis has now been approached
               more carefully than earlier. A more detailed discussion of the
               eigenvectors and eigenvalues of symmetric matrices, which are
               the type usually encountered in statistics, has been included in
               Sect. 1.1.3: also, new probabilistic concepts, such as factor
               analysis, have been discussed in Sect. 1.3.1. A survey of
               projection methods (Sect. 1.3.2) has been added, in order to
               relate the SOM to classical paradigms. Vector Quantization is
               now discussed in one main section, and derivation of the point
               density of the codebook vectors using the calculus of variations
               has been added, in order to familiarize the reader with this
               otherwise com plicated statistical analysis. It was also felt
               that the discussion of the neural-modeling philosophy should
               include a broader perspective of the main issues. A historical
               review in Sect. 2.2, and the general philosophy in Sects. 2.3,
               2.5 and 2.14 are now expected to especially help newcomers to
               orient themselves better amongst the profusion of contemporary
               neural models.",
  publisher = "Springer Science \& Business Media",
  month     =  dec,
  year      =  2012,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kohonen1990-ii,
  title     = "The self-organizing map",
  author    = "Kohonen, Teuvo",
  abstract  = "The self-organized map , an architecture suggested for
               artificial neural networks, is explained by presenting
               simulation experiments and practical applications. The self -
               organizing map has the property of effectively creating
               spatially organized internal representations of various …",
  journal   = "Proc. IEEE",
  publisher = "IEEE",
  volume    =  78,
  number    =  9,
  pages     = "1464--1480",
  year      =  1990
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Shepard1957-nn,
  title       = "{MULTIDIMENSIONAL-SCALING} {OF} {CONCEPTS} {BASED} {UPON}
                 {SEQUENCES} {OF} {RESTRICTED} {ASSOCIATIVE} {RESPONSES}",
  booktitle   = "American Psychologist",
  author      = "Shepard, R N",
  publisher   = "AMER PSYCHOLOGICAL ASSOC …",
  volume      =  12,
  pages       = "440--441",
  institution = "AMER PSYCHOLOGICAL ASSOC 750 FIRST ST NE, WASHINGTON, DC
                 20002-4242",
  year        =  1957
}

@ARTICLE{Shepard1962-yn,
  title     = "The analysis of proximities: Multidimensional scaling with an
               unknown distance function. {II}",
  author    = "Shepard, Roger N",
  abstract  = "The first in the present series of two papers described a
               computer program for multidimensional scaling on the basis of
               essentially nonmetric data. This second paper reports the
               results of two kinds of test applications of that program. The
               first application is to artificial data generated by
               monotonically transforming the interpoint distances in a known
               spatial configuration. The purpose is to show that the recovery
               of the original metric configuration does not depend upon the
               particular transformation used. The second application is to
               measures of interstimulus similarity and confusability obtained
               from some actual psychological experiments.",
  journal   = "Psychometrika",
  publisher = "Springer",
  volume    =  27,
  number    =  3,
  pages     = "219--246",
  month     =  sep,
  year      =  1962
}

@ARTICLE{Kruskal1971-ye,
  title     = "Monotone regression: Continuity and differentiability properties",
  author    = "Kruskal, J B",
  abstract  = "Least-squares monotone regression has received considerable
               discussion and use. Consider the residual sum of squaresQ
               obtained from the least-squares monotone regression ofyionxi.
               TreatingQ as a function of theyi, we prove that the gradient
               $\Delta$Q exists and is continuous everywhere, and is given by a
               simple formula. (We also discuss the gradient ofd=Q1/2.) These
               facts, which can be questioned (Louis Guttman, private
               communication), are important for the iterative numerical
               solution of models, such as some kinds of multidimensional
               scaling, in which monotone regression occurs as a subsidiary
               element, so that theyiand hence indirectlyQ are functions of
               other variables.",
  journal   = "Psychometrika",
  publisher = "Springer",
  volume    =  36,
  number    =  1,
  pages     = "57--62",
  month     =  mar,
  year      =  1971
}

@ARTICLE{Kruskal1977-vq,
  title     = "Multidimensional scaling and other methods for discovering
               structure",
  author    = "Kruskal, Joseph B and {Others}",
  journal   = "Statistical methods for digital computers",
  publisher = "Wiley New York",
  volume    =  3,
  pages     = "296--339",
  year      =  1977
}

@ARTICLE{Shepard1962-ac,
  title    = "The analysis of proximities: Multidimensional scaling with an
              unknown distance function. {I}",
  author   = "Shepard, Roger N",
  abstract = "A computer program is described that is designed to reconstruct
              the metric configuration of a set of points in Euclidean space on
              the basis of essentially nonmetric information about that
              configuration. A minimum set of Cartesian coordinates for the
              points is determined when the only available information
              specifies for each pair of those points---not the distance
              between them---but some unknown, fixed monotonic function of that
              distance. The program is proposed as a tool for reductively
              analyzing several types of psychological data, particularly
              measures of interstimulus similarity or confusability, by making
              explicit the multidimensional structure underlying such data.",
  journal  = "Psychometrika",
  volume   =  27,
  number   =  2,
  pages    = "125--140",
  month    =  jun,
  year     =  1962
}

@ARTICLE{Shepard1957-qv,
  title    = "Stimulus and response generalization: A stochastic model relating
              generalization to distance in psychological space",
  author   = "Shepard, Roger N",
  abstract = "A mathematical model is developed in an attempt to relate errors
              in multiple stimulus-response situations to psychological
              inter-stimulus and inter response distances. The fundamental
              assumptions are (a) that the stimulus and response confusions go
              on independently of each other, (b) that the probability of a
              stimulus confusion is an exponential decay function of the
              psychological distance between the stimuli, and (c) that the
              probability of a response confusion is an exponential decay
              function of the psychological distance between the responses. The
              problem of the operational definition of psychological distance
              is considered in some detail.",
  journal  = "Psychometrika",
  volume   =  22,
  number   =  4,
  pages    = "325--345",
  month    =  dec,
  year     =  1957
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Torgerson1958-lg,
  title        = "{PsycNET}",
  author       = "Torgerson, W S",
  abstract     = "A review and summarization of the literature on psychological
                  scaling prepared at the request of the Committee on Scaling
                  Theory and Methods of the Social Science Research Council.
                  Chapters dealing with the importance of measurement in
                  science, the nature of …",
  publisher    = "psycnet.apa.org",
  year         =  1958,
  howpublished = "\url{https://psycnet.apa.org/record/1959-07320-000}",
  note         = "Accessed: 2019-3-22"
}

@ARTICLE{Best1990-sz,
  title    = "Active set algorithms for isotonic regression; A unifying
              framework",
  author   = "Best, Michael J and Chakravarti, Nilotpal",
  abstract = "In this and subsequent papers we will show that several
              algorithms for the isotonic regression problem may be viewed as
              active set methods. The active set approach provides a unifying
              framework for studying algorithms for isotonic regression,
              simplifies the exposition of existing algorithms and leads to
              several new efficient algorithms. We also investigate the
              computational complexity of several algorithms.",
  journal  = "Math. Program.",
  volume   =  47,
  number   =  1,
  pages    = "425--439",
  month    =  may,
  year     =  1990
}

@ARTICLE{De_Leeuw2009-oo,
  title    = "Isotone Optimization in R: {Pool-Adjacent-Violators} Algorithm
              ({PAVA}) and Active Set Methods",
  author   = "de Leeuw, Jan and Hornik, Kurt and Mair, Patrick",
  abstract = "In this paper we give a general framework for isotone
              optimization. First we discuss a generalized version of the
              pool-adjacent-violators algorithm (PAVA) to minimize a separable
              convex function with simple chain constraints. Besides of general
              convex functions we extend existing PAVA implementations in terms
              of observation weights, approaches for tie handling, and
              responses from repeated measurement designs. Since isotone
              optimization problems can be formulated as convex programming
              problems with linear constraints we the develop a primal active
              set method to solve such problem. This methodology is applied on
              specific loss functions relevant in statistics. Both approaches
              are implemented in the R package isotone.",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  32,
  number   =  5,
  pages    = "1--24",
  year     =  2009
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{De_Leeuw2011-xb,
  title     = "Multidimensional Scaling Using Majorization: {SMACOF} in {R}",
  author    = "de Leeuw, Jan and Mair, Patrick",
  abstract  = "Author(s): Jan de Leeuw; Patrick Mair | Abstract: In this paper
               we present the methodology of multidimensional scaling problems
               (MDS) solved by means of the majorization algorithm. The
               objective function to be minimized is known as stress and
               functions which majorize stress are elaborated. This strategy to
               solve MDS problems is called SMACOF and it is implemented in an
               R package of the same name which is presented in this article.
               We extend the basic SMACOF theory in terms of conﬁguration
               constraints, three-way data, unfolding models, and projection of
               the resulting conﬁgurations onto spheres and other quadratic
               surfaces. Various examples are presented to show the
               possibilities of the SMACOF approach oﬀered by the corresponding
               package.",
  publisher = "escholarship.org",
  month     =  oct,
  year      =  2011,
  keywords  = "Physical Sciences and Mathematics"
}

@ARTICLE{Kruskal1964-md,
  title     = "Nonmetric multidimensional scaling: A numerical method",
  author    = "Kruskal, J B",
  abstract  = "We describe the numerical methods required in our approach to
               multi-dimensional scaling. The rationale of this approach has
               appeared previously.",
  journal   = "Psychometrika",
  publisher = "Springer-Verlag",
  volume    =  29,
  number    =  2,
  pages     = "115--129",
  month     =  jun,
  year      =  1964,
  language  = "en"
}

@ARTICLE{Kruskal1964-iv,
  title     = "Multidimensional scaling by optimizing goodness of fit to a
               nonmetric hypothesis",
  author    = "Kruskal, J B",
  abstract  = "Multidimensional scaling is the problem of representingn objects
               geometrically byn points, so that the interpoint distances
               correspond in some sense to experimental dissimilarities between
               objects. In just what sense distances and dissimilarities should
               correspond has been left rather vague in most approaches, thus
               leaving these approaches logically incomplete. Our fundamental
               hypothesis is that dissimilarities and distances are
               monotonically related. We define a quantitative, intuitively
               satisfying measure of goodness of fit to this hypothesis. Our
               technique of multidimensional scaling is to compute that
               configuration of points which optimizes the goodness of fit. A
               practical computer program for doing the calculations is
               described in a companion paper.",
  journal   = "Psychometrika",
  publisher = "Springer",
  volume    =  29,
  number    =  1,
  pages     = "1--27",
  month     =  mar,
  year      =  1964
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Borg2003-du,
  title     = "Modern Multidimensional Scaling: Theory and Applications",
  author    = "Borg, I and Groenen, P",
  abstract  = "Multidimensional scaling (MDS) is an extremely general scaling
               procedure that has seen little application in educational
               measurement. This lack of application is unfortunate because,
               among its other uses, MDS can help us better understand what
               educational tests …",
  journal   = "J Educational Measurement",
  publisher = "Wiley Online Library",
  volume    =  40,
  number    =  3,
  pages     = "277--280",
  month     =  sep,
  year      =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MASTERSTHESIS{Smit2018-jo,
  title     = "Avoiding local minima in nonmetric multidimensional scaling by
               means of deterministic annealing and iterated local search",
  author    = "Smit, Janneke and {Others}",
  abstract  = "The current study aimed to map the seriousness of the local
               minima problem in nonmetric multidimensional scaling (MDS) by
               evaluating the results of the single random start procedure
               (SR). Moreover, Deterministic annealing (DA) and iterated local
               search (ILS) …",
  publisher = "openaccess.leidenuniv.nl",
  year      =  2018
}

@ARTICLE{Gower1966-es,
  title     = "Some Distance Properties of Latent Root and Vector Methods Used
               in Multivariate Analysis",
  author    = "Gower, J C",
  abstract  = "[This paper is concerned with the representation of a
               multivariate sample of size $n$ as points $P_1, P_2,\ldots, P_n$
               in a Euclidean space. The interpretation of the distance
               $\Delta(P_i, P_j)$ between the $i$th and $j$th members of the
               sample is discussed for some commonly used types of analysis,
               including both $Q$ and $R$ techniques. When all the distances
               between $n$ points are known a method is derived which finds
               their co-ordinates referred to principal axes. A set of
               necessary and sufficient conditions for a solution to exist in
               real Euclidean space is found. $Q$ and $R$ techniques are
               defined as being dual to one another when they both lead to a
               set of $n$ points with the same inter-point distances. Pairs of
               dual techniques are derived. In factor analysis the distances
               between points whose co-ordinates are the estimated factor
               scores can be interpreted as $D^2$ with a singular dispersion
               matrix.]",
  journal   = "Biometrika",
  publisher = "[Oxford University Press, Biometrika Trust]",
  volume    =  53,
  number    = "3/4",
  pages     = "325--338",
  year      =  1966
}




% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Baker1977-eq,
  title     = "The numerical treatment of integral equations",
  author    = "Baker, Christopher T H",
  abstract  = "CERN Accelerating science. Sign in; Directory. CERN Document
               Server. Access articles, reports and multimedia content in HEP.
               Main menu. Search; Submit; Help; Personalize: Your alerts; Your
               baskets; Your comments; Your searches. Home > The numerical
               treatment of integral equations. Information; Discussion (0);
               Files; Holdings. Book. Title, The numerical treatment of
               integral equations. Author(s), Baker, Christopher TH.
               Publication, Oxford : Clarendon Press, 1977. - 1034 p. Series,
               (Mono. Num. Analys.). Subject code, 517.968. Subject category,
               Mathematical Physics …",
  publisher = "Clarendon press",
  year      =  1977
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Shawe-Taylor2003-ux,
  title     = "The stability of kernel principal components analysis and its
               relation to the process eigenspectrum",
  author    = "Shawe-Taylor, John and Williams, Christopher K I",
  abstract  = "In this paper we analyze the relationships between the
               eigenvalues of the mxm Gram matrix K for a kernel k (·,.)
               corresponding to a sample Xl,..., Xm drawn from a density p (x)
               and the eigenvalues of the corresponding continuous
               eigenproblem. We bound the dif-ferences between the two spectra
               and provide a performance bound on kernel peA.",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "MIT; 1998",
  pages     = "383--390",
  year      =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gower1968-ow,
  title     = "Adding a point to vector diagrams in multivariate analysis",
  author    = "Gower, J C",
  abstract  = "Abstract. A set of n base points P4i=1, 2,…, n), with known
               co-ordinates relative to orthogonal axes, and a further point
               Pn+1, with known distance from each o",
  journal   = "Biometrika",
  publisher = "Oxford Academic",
  volume    =  55,
  number    =  3,
  pages     = "582--585",
  month     =  nov,
  year      =  1968
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dempster1977-jl,
  title     = "Maximum likelihood from incomplete data via the {EM} algorithm",
  author    = "Dempster, Arthur P and Laird, Nan M and Rubin, Donald B",
  abstract  = "A broadly applicable algorithm for computing maximum likelihood
               estimates from incomplete data is presented at various levels of
               generality. Theory showing the monotone behaviour of the
               likelihood and convergence of the algorithm is derived. Many
               examples are sketched …",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley Online Library",
  volume    =  39,
  number    =  1,
  pages     = "1--22",
  year      =  1977
}



% Manifold learning evaluation

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Dollar2007-gr,
  title     = "Learning to Traverse Image Manifolds",
  booktitle = "Advances in Neural Information Processing Systems 19",
  author    = "Doll{\'a}r, Piotr and Rabaud, Vincent and Belongie, Serge J",
  editor    = "Sch{\"o}lkopf, B and Platt, J C and Hoffman, T",
  abstract  = "We present a new algorithm, Locally Smooth Manifold Learning
               (LSML), that learns a warping function from a point on an
               manifold to its neighbors. Important characteristics of LSML
               include the ability to recover the structure of the manifold in
               sparsely populated …",
  publisher = "MIT Press",
  pages     = "361--368",
  year      =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mordohai2010-pk,
  title     = "Dimensionality Estimation, Manifold Learning and Function
               Approximation using Tensor Voting",
  author    = "Mordohai, Philippos and Medioni, G{\'e}rard",
  abstract  = "We address instance-based learning from a perceptual
               organization standpoint and present methods for dimensionality
               estimation, manifold learning and function approximation. Under
               our approach, manifolds in high-dimensional spaces are inferred
               by estimating geometric relationships among the input instances.
               Unlike conventional manifold learning, we do not perform
               dimensionality reduction, but instead perform all operations in
               the original input space. For this purpose we employ a novel
               formulation of tensor voting, which allows an ND …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  11,
  number    = "Jan",
  pages     = "411--450",
  year      =  2010
}


% image processing

@ARTICLE{Weinberger2006-dc,
  title    = "Unsupervised Learning of Image Manifolds by Semidefinite
              Programming",
  author   = "Weinberger, Kilian Q and Saul, Lawrence K",
  abstract = "Can we detect low dimensional structure in high dimensional data
              sets of images? In this paper, we propose an algorithm for
              unsupervised learning of image manifolds by semidefinite
              programming. Given a data set of images, our algorithm computes a
              low dimensional representation of each image with the property
              that distances between nearby images are preserved. More
              generally, it can be used to analyze high dimensional data that
              lies on or near a low dimensional manifold. We illustrate the
              algorithm on easily visualized examples of curves and surfaces,
              as well as on actual images of faces, handwritten digits, and
              solid objects.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  70,
  number   =  1,
  pages    = "77--90",
  month    =  oct,
  year     =  2006
}

@INPROCEEDINGS{Elgammal2004-wj,
  title     = "Inferring {3D} body pose from silhouettes using activity
               manifold learning",
  booktitle = "Proceedings of the 2004 {IEEE} Computer Society Conference on
               Computer Vision and Pattern Recognition, 2004. {CVPR} 2004.",
  author    = "Elgammal, A and {Chan-Su Lee}",
  abstract  = "We aim to infer 3D body pose directly from human silhouettes.
               Given a visual input (silhouette), the objective is to recover
               the intrinsic body configuration, recover the viewpoint,
               reconstruct the input and detect any spatial or temporal
               outliers. In order to recover intrinsic body configuration
               (pose) from the visual input (silhouette), we explicitly learn
               view-based representations of activity manifolds as well as
               learn mapping functions between such central representations and
               both the visual input space and the 3D body pose space. The body
               pose can be recovered in a closed form in two steps by
               projecting the visual input to the learned representations of
               the activity manifold, i.e., finding the point on the learned
               manifold representation corresponding to the visual input,
               followed by interpolating 3D pose.",
  volume    =  2,
  pages     = "II--II",
  month     =  jun,
  year      =  2004,
  keywords  = "multidimensional signal processing;image representation;image
               reconstruction;image motion analysis;3D body pose;activity
               manifold learning;human silhouettes;intrinsic body
               configuration;view-based representations;intrinsic body
               recovery;mapping functions;Humans;Image reconstruction;Motion
               analysis;Solid modeling;Kinematics;Cameras;Computer
               science;Surveillance;Biological system modeling;Geometry"
}


@ARTICLE{Zhu2018-jw,
  title    = "Image reconstruction by domain-transform manifold learning",
  author   = "Zhu, Bo and Liu, Jeremiah Z and Cauley, Stephen F and Rosen,
              Bruce R and Rosen, Matthew S",
  abstract = "Image reconstruction is essential for imaging applications across
              the physical and life sciences, including optical and radar
              systems, magnetic resonance imaging, X-ray computed tomography,
              positron emission tomography, ultrasound imaging and radio
              astronomy. During image acquisition, the sensor encodes an
              intermediate representation of an object in the sensor domain,
              which is subsequently reconstructed into an image by an inversion
              of the encoding function. Image reconstruction is challenging
              because analytic knowledge of the exact inverse transform may not
              exist a priori, especially in the presence of sensor
              non-idealities and noise. Thus, the standard reconstruction
              approach involves approximating the inverse function with
              multiple ad hoc stages in a signal processing chain, the
              composition of which depends on the details of each acquisition
              strategy, and often requires expert parameter tuning to optimize
              reconstruction performance. Here we present a unified framework
              for image reconstruction-automated transform by manifold
              approximation (AUTOMAP)-which recasts image reconstruction as a
              data-driven supervised learning task that allows a mapping
              between the sensor and the image domain to emerge from an
              appropriate corpus of training data. We implement AUTOMAP with a
              deep neural network and exhibit its flexibility in learning
              reconstruction transforms for various magnetic resonance imaging
              acquisition strategies, using the same network architecture and
              hyperparameters. We further demonstrate that manifold learning
              during training results in sparse representations of domain
              transforms along low-dimensional data manifolds, and observe
              superior immunity to noise and a reduction in reconstruction
              artefacts compared with conventional handcrafted reconstruction
              methods. In addition to improving the reconstruction performance
              of existing acquisition methodologies, we anticipate that AUTOMAP
              and other learned reconstruction approaches will accelerate the
              development of new acquisition strategies across imaging
              modalities.",
  journal  = "Nature",
  volume   =  555,
  number   =  7697,
  pages    = "487--492",
  month    =  mar,
  year     =  2018,
  language = "en"
}

% anomaly detection

@inproceedings{Neal_Patwari2005,
author = {Patwari, Neal and Hero, Alfred O. and Pacholski, Adam},
title = {Manifold Learning Visualization of Network Traffic Data},
year = {2005},
isbn = {1595930264},
doi = {10.1145/1080173.1080182},
booktitle = {Proceedings of the 2005 ACM SIGCOMM Workshop on Mining Network Data},
pages = {191–196},
numpages = {6},
keywords = {internet traffic anomaly detection & forensics, data mining},
series = {MineNet ’05}
}



@ARTICLE{Olson2018-nt,
  title    = "Manifold learning techniques for unsupervised anomaly detection",
  author   = "Olson, C C and Judd, K P and Nichols, J M",
  abstract = "Appropriately identifying outlier data is a critical requirement
              in the decision-making process of many expert and intelligent
              systems deployed in a variety of fields including finance,
              medicine, and defense. Classical outlier detection schemes
              typically rely on the assumption that normal/background data of
              interest are distributed according to an assumed statistical
              model and search for data that deviate from that assumption.
              However, it is frequently the case that performance is reduced
              because the underlying distribution does not follow the assumed
              model. Manifold learning techniques offer improved performance by
              learning better models of the background but can be too
              computationally expensive due to the need to calculate a distance
              measure between all data points. Here, we study a general
              framework that allows manifold learning techniques to be used for
              unsupervised anomaly detection by reducing computational expense
              via a uniform random sampling of a small fraction of the data. A
              background manifold is learned from the sample and then an
              out-of-sample extension is used to project unsampled data into
              the learned manifold space and construct an anomaly detection
              statistic based on the prediction error of the learned manifold.
              The method works well for unsupervised anomaly detection because,
              by definition, the ratio of anomalous to non-anomalous data
              points is small and the sampling will be dominated by background
              points. However, a variety of parameters that affect detection
              performance are introduced so we use here a low-dimensional toy
              problem to investigate their effect on the performance of four
              learning algorithms (kernel PCA, two versions of diffusion map,
              and the Parzen density estimator). We then apply the methods to
              the detection of watercraft in an ensemble of 22 infrared
              maritime scenes where we find kernel PCA to be superior and show
              that it outperforms a commonly employed baseline algorithm. The
              framework is not limited to the tested image processing example
              and can be used for any unsupervised anomaly detection task.",
  journal  = "Expert Syst. Appl.",
  volume   =  91,
  pages    = "374--385",
  month    =  jan,
  year     =  2018,
  keywords = "Manifolds; Manifold learning; Image processing; Anomaly
              detection; Target detection"
}


% depth first search

@ARTICLE{Tarjan1972-kj,
  title     = "{Depth-First} Search and Linear Graph Algorithms",
  author    = "Tarjan, Robert",
  abstract  = "The value of depth-first search or ?backtracking? as a technique
               for solving problems is illustrated by two examples. An improved
               version of an algorithm for finding the strongly connected
               components of a directed graph and at algorithm for finding the
               biconnected components of an undirect graph are presented. The
               space and time requirements of both algorithms are bounded by
               $k_1 V + k_2 E + k_3 $ for some constants $k_1 ,k_2 $, and $k_3
               $, where V is the number of vertices and E is the number of
               edges of the graph being examined.",
  journal   = "SIAM J. Comput.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  1,
  number    =  2,
  pages     = "146--160",
  month     =  jun,
  year      =  1972
}



% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kraemer2018-zf,
  title     = "dimRed and {coRanking---Unifying} dimensionality reduction in
               {R}",
  author    = "Kraemer, Guido and Reichstein, Markus and Mahecha, Miguel D",
  abstract  = "Dimensionality reduction''(DR) is a widely used approach to find
               low dimensional and interpretable representations of data that
               are natively embedded in high-dimensional spaces. DR can be
               realized by a plethora of methods with different properties,
               objectives …",
  journal   = "The R Journal",
  publisher = "R Foundation",
  volume    =  10,
  number    =  1,
  pages     = "342--358",
  year      =  2018
}



@INPROCEEDINGS{Zhang2003-yi,
  title     = "Nonlinear Dimension Reduction via Local Tangent Space Alignment",
  booktitle = "Intelligent Data Engineering and Automated Learning",
  author    = "Zhang, Zhenyue and Zha, Hongyuan",
  abstract  = "In this paper we present a new algorithm for manifold learning
               and nonlinear dimension reduction. Based on a set of unorganized
               data points sampled with noise from the manifold, we represent
               the local geometry of the manifold using tangent spaces learned
               by fitting an affine subspace in a neighborhood of each data
               point. Those tangent spaces are aligned to give the internal
               global coordinates of the data points with respect to the
               underlying manifold by way of a partial eigendecomposition of
               the neighborhood connection matrix. We present a careful error
               analysis of our algorithm and show that the reconstruction
               errors are of second-order accuracy. Numerical experiments
               including 64-by-64 pixel face images are given to illustrate our
               algorithm.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "477--481",
  year      =  2003
}



@ARTICLE{Coifman2006-no,
  title     = "Diffusion maps",
  author    = "Coifman, Ronald R and Lafon, St{\'e}phane",
  abstract  = "In this paper, we provide a framework based upon diffusion
               processes for finding meaningful geometric descriptions of data
               sets. We show that eigenfunctions of Markov matrices can be used
               to construct coordinates called diffusion maps that generate
               efficient representations of complex geometric structures. The
               associated family of diffusion distances, obtained by iterating
               the Markov matrix, defines multiscale geometries that prove to
               be useful in the context of data parametrization and
               dimensionality reduction. The proposed framework relates the
               spectral properties of Markov processes to their geometric
               counterparts and it unifies ideas arising in a variety of
               contexts such as machine learning, spectral graph theory and
               eigenmap methods.",
  journal   = "Appl. Comput. Harmon. Anal.",
  publisher = "Elsevier",
  volume    =  21,
  number    =  1,
  pages     = "5--30",
  month     =  jul,
  year      =  2006,
  keywords  = "Diffusion processes; Diffusion metric; Manifold learning;
               Dimensionality reduction; Eigenmaps; Graph Laplacian"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Nadler2006-cm,
  title     = "Diffusion Maps, Spectral Clustering and Eigenfunctions of
               {Fokker-Planck} Operators",
  booktitle = "Advances in Neural Information Processing Systems 18",
  author    = "Nadler, Boaz and Lafon, Stephane and Kevrekidis, Ioannis and
               Coifman, Ronald R",
  editor    = "Weiss, Y and Sch{\"o}lkopf, B and Platt, J C",
  abstract  = "This paper presents a diffusion based probabilistic
               interpretation of spectral clustering and dimensionality
               reduction algorithms that use the eigenvectors of the normalized
               graph Laplacian. Given the pairwise adjacency matrix of all
               points, we define a diffusion distance …",
  publisher = "MIT Press",
  pages     = "955--962",
  year      =  2006
}


@ARTICLE{Hoffmann2007-kh,
  title     = "Kernel {PCA} for novelty detection",
  author    = "Hoffmann, Heiko",
  abstract  = "Kernel principal component analysis (kernel PCA) is a non-linear
               extension of PCA. This study introduces and investigates the use
               of kernel PCA for novelty detection. Training data are mapped
               into an infinite-dimensional feature space. In this space,
               kernel PCA extracts the principal components of the data
               distribution. The squared distance to the corresponding
               principal subspace is the measure for novelty. This new method
               demonstrated a competitive performance on two-dimensional
               synthetic distributions and on two real-world data sets:
               handwritten digits and breast-cancer cytology.",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier",
  volume    =  40,
  number    =  3,
  pages     = "863--874",
  month     =  mar,
  year      =  2007,
  keywords  = "Kernel method; Novelty detection; PCA; Handwritten digit; Breast
               cancer"
}

@ARTICLE{Kwang_In_Kim2002-ly,
  title     = "Face recognition using kernel principal component analysis",
  author    = "{Kwang In Kim} and {Keechul Jung} and {Hang Joon Kim}",
  abstract  = "A kernel principal component analysis (PCA) was previously
               proposed as a nonlinear extension of a PCA. The basic idea is to
               first map the input space into a feature space via nonlinear
               mapping and then compute the principal components in that
               feature space. This article adopts the kernel PCA as a mechanism
               for extracting facial features. Through adopting a polynomial
               kernel, the principal components can be computed within the
               space spanned by high-order correlations of input pixels making
               up a facial image, thereby producing a good performance.",
  journal   = "IEEE Signal Process. Lett.",
  publisher = "ieeexplore.ieee.org",
  volume    =  9,
  number    =  2,
  pages     = "40--42",
  month     =  feb,
  year      =  2002,
  keywords  = "face recognition;feature extraction;polynomials;correlation
               methods;principal component analysis;face recognition;kernel
               principal component analysis;kernel PCA;input space;feature
               space;nonlinear mapping;polynomial kernel;high-order
               correlations;input pixels;facial image;face feature
               extraction;Face recognition;Kernel;Principal component
               analysis;Eigenvalues and eigenfunctions;Pixel;Data
               mining;Laboratories;Polynomials;Artificial intelligence;Computer
               science"
}

@INPROCEEDINGS{Scholkopf1997-uq,
  title     = "Kernel principal component analysis",
  booktitle = "Artificial Neural Networks --- {ICANN'97}",
  author    = "Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller,
               Klaus-Robert",
  abstract  = "A new method for performing a nonlinear form of Principal
               Component Analysis is proposed. By the use of integral operator
               kernel functions, one can efficiently compute principal
               components in highdimensional feature spaces, related to input
               space by some nonlinear map; for instance the space of all
               possible d-pixel products in images. We give the derivation of
               the method and present experimental results on polynomial
               feature extraction for pattern recognition.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "583--588",
  year      =  1997
}


@ARTICLE{Osan2018-ec,
  title    = "Monoparametric family of metrics derived from classical
              {Jensen--Shannon} divergence",
  author   = "Os{\'a}n, Trist{\'a}n M and Bussandri, Diego G and Lamberti,
              Pedro W",
  abstract = "Jensen--Shannon divergence is a well known multi-purpose measure
              of dissimilarity between probability distributions. It has been
              proven that the square root of this quantity is a true metric in
              the sense that, in addition to the basic properties of a
              distance, it also satisfies the triangle inequality. In this work
              we extend this last result to prove that in fact it is possible
              to derive a monoparametric family of metrics from the classical
              Jensen--Shannon divergence. Motivated by our results, an
              application into the field of symbolic sequences segmentation is
              explored. Additionally, we analyze the possibility to extend this
              result into the quantum realm.",
  journal  = "Physica A: Statistical Mechanics and its Applications",
  volume   =  495,
  pages    = "336--344",
  month    =  apr,
  year     =  2018,
  keywords = "Jensen--Shannon divergence; Metrics; Information theory; Quantum
              distances"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kullback1951-gi,
  title     = "On Information and Sufficiency",
  author    = "Kullback, S and Leibler, R A",
  abstract  = "1. Introduction. This note generalizes to the abstract case
               Shannon's definition of information 115],[161. Wiener's
               information (p. 75 of [18)) is essentially the same as Shannon's
               although their motivation was different (cf. footnote 1, p. 95
               of [161) and Shannon apparently has …",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  22,
  number    =  1,
  pages     = "79--86",
  year      =  1951
}


@ARTICLE{FloydRobert1962-au,
  title     = "Algorithm 97: Shortest path",
  author    = "Floyd, Robert, W",
  journal   = "Commun. ACM",
  publisher = "ACM PUB27 New York, NY, USA",
  month     =  jun,
  year      =  1962,
  language  = "en"
}

@ARTICLE{Dijkstra1959-ml,
  title   = "A note on two problems in connexion with graphs",
  author  = "Dijkstra, E W",
  journal = "Numer. Math.",
  volume  =  1,
  number  =  1,
  pages   = "269--271",
  month   =  dec,
  year    =  1959
}


@ARTICLE{Lawrence2012-cz,
  title    = "A Unifying Probabilistic Perspective for Spectral Dimensionality
              Reduction: Insights and New Models",
  author   = "Lawrence, Neil D",
  journal  = "J. Mach. Learn. Res.",
  volume   =  13,
  number   = "May",
  pages    = "1609--1638",
  year     =  2012
}



% 20200331

@ARTICLE{Goldberg2009-tb,
  title    = "Local procrustes for manifold embedding: a measure of embedding
              quality and embedding algorithms",
  author   = "Goldberg, Yair and Ritov, Ya'acov",
  abstract = "We present the Procrustes measure, a novel measure based on
              Procrustes rotation that enables quantitative comparison of the
              output of manifold-based embedding algorithms such as LLE (Roweis
              and Saul, Science 290(5500), 2323--2326, 2000) and Isomap
              (Tenenbaum et al., Science 290(5500), 2319--2323, 2000). The
              measure also serves as a natural tool when choosing
              dimension-reduction parameters. We also present two novel
              dimension-reduction techniques that attempt to minimize the
              suggested measure, and compare the results of these techniques to
              the results of existing algorithms. Finally, we suggest a simple
              iterative method that can be used to improve the output of
              existing algorithms.",
  journal  = "Mach. Learn.",
  volume   =  77,
  number   =  1,
  pages    = "1--25",
  month    =  oct,
  year     =  2009
}



@ARTICLE{Chen2009-su,
  title     = "Local Multidimensional Scaling for Nonlinear Dimension
               Reduction, Graph Drawing, and Proximity Analysis",
  author    = "Chen, Lisha and Buja, Andreas",
  abstract  = "In the past decade there has been a resurgence of interest in
               nonlinear dimension reduction. Among new proposals are ?Local
               Linear Embedding,? ?Isomap,? and Kernel Principal Components
               Analysis which all construct global low-dimensional embeddings
               from local affine or metric information. We introduce a
               competing method called ?Local Multidimensional Scaling? (LMDS).
               Like LLE, Isomap, and KPCA, LMDS constructs its global embedding
               from local information, but it uses instead a combination of MDS
               and ?force-directed? graph drawing. We apply the force paradigm
               to create localized versions of MDS stress functions with a
               tuning parameter to adjust the strength of nonlocal repulsive
               forces. We solve the problem of tuning parameter selection with
               a meta-criterion that measures how well the sets of K-nearest
               neighbors agree between the data and the embedding. Tuned LMDS
               seems to be able to outperform MDS, PCA, LLE, Isomap, and KPCA,
               as illustrated with two well-known image datasets. The
               meta-criterion can also be used in a pointwise version as a
               diagnostic tool for measuring the local adequacy of embeddings
               and thereby detect local problems in dimension reductions.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  104,
  number    =  485,
  pages     = "209--219",
  month     =  mar,
  year      =  2009
}

@ARTICLE{Venna2006-nd,
  title    = "Local multidimensional scaling",
  author   = "Venna, Jarkko and Kaski, Samuel",
  abstract = "In a visualization task, every nonlinear projection method needs
              to make a compromise between trustworthiness and continuity. In a
              trustworthy projection the visualized proximities hold in the
              original data as well, whereas a continuous projection visualizes
              all proximities of the original data. We show experimentally that
              one of the multidimensional scaling methods, curvilinear
              components analysis, is good at maximizing trustworthiness. We
              then extend it to focus on local proximities both in the input
              and output space, and to explicitly make a user-tunable
              parameterized compromise between trustworthiness and continuity.
              The new method compares favorably to alternative nonlinear
              projection methods.",
  journal  = "Neural Netw.",
  volume   =  19,
  number   = "6-7",
  pages    = "889--899",
  month    =  jul,
  year     =  2006,
  language = "en"
}



@ARTICLE{Chen2019-bh,
  title         = "Selecting the independent coordinates of manifolds with
                   large aspect ratios",
  author        = "Chen, Yu-Chia and Meil{\u a}, Marina",
  abstract      = "Many manifold embedding algorithms fail apparently when the
                   data manifold has a large aspect ratio (such as a long, thin
                   strip). Here, we formulate success and failure in terms of
                   finding a smooth embedding, showing also that the problem is
                   pervasive and more complex than previously recognized.
                   Mathematically, success is possible under very broad
                   conditions, provided that embedding is done by carefully
                   selected eigenfunctions of the Laplace-Beltrami operator
                   $\Delta$. Hence, we propose a bicriterial Independent
                   Eigencoordinate Selection (IES) algorithm that selects
                   smooth embeddings with few eigenvectors. The algorithm is
                   grounded in theory, has low computational overhead, and is
                   successful on synthetic and large real data.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1907.01651"
}

@ARTICLE{Perraul-Joncas2013-to,
  title     = "Non-linear dimensionality reduction: Riemannian metric
               estimation and the problem of geometric discovery",
  author    = "Perraul-Joncas, Dominique and Meila, Marina",
  abstract  = "Download Citation | Non-linear dimensionality reduction:
               Riemannian metric estimation and the problem of geometric
               discovery | In recent years, manifold learning has become
               increasingly popular as a tool for performing non-linear
               dimensionality reduction. This has led to... | Find, read and
               cite all the research you need on ResearchGate",
  publisher = "unknown",
  month     =  may,
  year      =  2013
}



@MISC{Perraul-Joncas2012-pv,
  title        = "Metric learning of manifolds",
  author       = "{Perraul-Joncas, D , Meila,}",
  year         =  2012,
  howpublished = "ITA"
}




@ARTICLE{McQueen2016-re,
  title         = "megaman: Manifold Learning with Millions of points",
  author        = "McQueen, James and Meila, Marina and VanderPlas, Jacob and
                   Zhang, Zhongyue",
  abstract      = "Manifold Learning is a class of algorithms seeking a
                   low-dimensional non-linear representation of
                   high-dimensional data. Thus manifold learning algorithms
                   are, at least in theory, most applicable to high-dimensional
                   data and sample sizes to enable accurate estimation of the
                   manifold. Despite this, most existing manifold learning
                   implementations are not particularly scalable. Here we
                   present a Python package that implements a variety of
                   manifold learning algorithms in a modular and scalable
                   fashion, using fast approximate neighbors searches and fast
                   sparse eigendecompositions. The package incorporates
                   theoretical advances in manifold learning, such as the
                   unbiased Laplacian estimator and the estimation of the
                   embedding distortion by the Riemannian metric method. In
                   benchmarks, even on a single-core desktop computer, our code
                   embeds millions of data points in minutes, and takes just
                   200 minutes to embed the main sample of galaxy spectra from
                   the Sloan Digital Sky Survey --- consisting of 0.6 million
                   samples in 3750-dimensions --- a task which has not
                   previously been possible.",
  month         =  mar,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1603.02763"
}

% 20200407


@ARTICLE{Gracia2014-ae,
  title    = "A methodology to compare Dimensionality Reduction algorithms in
              terms of loss of quality",
  author   = "Gracia, Antonio and Gonz{\'a}lez, Santiago and Robles, Victor and
              Menasalvas, Ernestina",
  abstract = "Dimensionality Reduction (DR) is attracting more attention these
              days as a result of the increasing need to handle huge amounts of
              data effectively. DR methods allow the number of initial features
              to be reduced considerably until a set of them is found that
              allows the original properties of the data to be kept. However,
              their use entails an inherent loss of quality that is likely to
              affect the understanding of the data, in terms of data analysis.
              This loss of quality could be determinant when selecting a DR
              method, because of the nature of each method. In this paper, we
              propose a methodology that allows different DR methods to be
              analyzed and compared as regards the loss of quality produced by
              them. This methodology makes use of the concept of preservation
              of geometry (quality assessment criteria) to assess the loss of
              quality. Experiments have been carried out by using the most
              well-known DR algorithms and quality assessment criteria, based
              on the literature. These experiments have been applied on 12
              real-world datasets. Results obtained so far show that it is
              possible to establish a method to select the most appropriate DR
              method, in terms of minimum loss of quality. Experiments have
              also highlighted some interesting relationships between the
              quality assessment criteria. Finally, the methodology allows the
              appropriate choice of dimensionality for reducing data to be
              established, whilst giving rise to a minimum loss of quality.",
  journal  = "Inf. Sci.",
  volume   =  270,
  pages    = "1--27",
  month    =  jun,
  year     =  2014,
  keywords = "Manifold learning; Nonlinear dimensionality reduction; Linear
              dimensionality reduction; Loss of quality; Quality assessment
              criteria"
}

@ARTICLE{Liang2017-qq,
  title         = "A New Method for Performance Analysis in Nonlinear
                   Dimensionality Reduction",
  author        = "Liang, Jiaxi and Chenouri, Shojaeddin and Small, Christopher
                   G",
  abstract      = "In this paper, we develop a local rank correlation measure
                   which quantifies the performance of dimension reduction
                   methods. The local rank correlation is easily interpretable,
                   and robust against the extreme skewness of nearest neighbor
                   distributions in high dimensions. Some benchmark datasets
                   are studied. We find that the local rank correlation closely
                   corresponds to our visual interpretation of the quality of
                   the output. In addition, we demonstrate that the local rank
                   correlation is useful in estimating the intrinsic
                   dimensionality of the original data, and in selecting a
                   suitable value of tuning parameters used in some algorithms.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1711.06252"
}

@INPROCEEDINGS{Venna2001-sg,
  title     = "Neighborhood Preservation in Nonlinear Projection Methods: An
               Experimental Study",
  booktitle = "Artificial Neural Networks --- {ICANN} 2001",
  author    = "Venna, Jarkko and Kaski, Samuel",
  abstract  = "Several measures have been proposed for comparing nonlinear
               projection methods but so far no comparisons have taken into
               account one of their most important properties, the
               trustworthiness of the resulting neighborhood or proximity
               relationships. One of the main uses of nonlinear mapping methods
               is to visualize multivariate data, and in such visualizations it
               is crucial that the visualized proximities can be trusted upon:
               If two data samples are close to each other on the display they
               should be close-by in the original space as well. A local
               measure of trustworthiness is proposed and it is shown for three
               data sets that neighborhood relationships visualized by the
               Self-Organizing Map and its variant, the Generative Topographic
               Mapping, are more trustworthy than visualizations produced by
               traditional multidimensional scaling-based nonlinear projection
               methods.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "485--491",
  year      =  2001
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Goldberg2008-co,
  title     = "Manifold Learning: The Price of Normalization",
  author    = "Goldberg, Yair and Zakai, Alon and Kushnir, Dan and Ritov,
               Ya'acov",
  abstract  = "We analyze the performance of a class of manifold-learning
               algorithms that find their output by minimizing a quadratic form
               under some normalization constraints. This class consists of
               Locally Linear Embedding (LLE), Laplacian Eigenmap, Local
               Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and
               Diffusion maps. We present and prove conditions on the manifold
               that are necessary for the success of the algorithms. Both the
               finite sample case and the limit case are analyzed. We show that
               there are simple manifolds in which the …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  9,
  number    = "Aug",
  pages     = "1909--1939",
  year      =  2008
}

@INPROCEEDINGS{Venna2005-bc,
  title       = "Local multidimensional scaling with controlled tradeoff
                 between trustworthiness and continuity",
  booktitle   = "Proceedings of {WSOM}",
  author      = "Venna, Jarkko and Kaski, Samuel",
  abstract    = "In a visualization task, every nonlinear projection method
                 needs to make a compromise between trustworthiness and
                 continuity. In a trustworthy projection the visualized
                 proximities hold in the original data as well, whereas a
                 continuous projection visualizes all proximities of the
                 original data. A multidimensional scaling method, curvilinear
                 components analysis, is good at maximizing trustworthiness. We
                 extend it to explicitly make a user-tunable parameterized
                 compromise between trustworthiness and continuity.",
  publisher   = "Citeseer",
  volume      =  5,
  pages       = "695--702",
  institution = "Citeseer",
  year        =  2005
}



@INPROCEEDINGS{Lee2008-cx,
  title     = "Quality assessment of nonlinear dimensionality reduction based
               on K-ary neighborhoods",
  booktitle = "New Challenges for Feature Selection in Data Mining and
               Knowledge Discovery",
  author    = "Lee, John and Verleysen, Michel",
  abstract  = "Nonlinear dimensionality reduction aims at providing
               low-dimensional representions of high- dimensional data sets.
               Many new methods have been recently proposed, but the question
               of their assessment and comparison remains open. This paper
               reviews some of the existing quality measures that are based on
               distance ranking and K-ary neighborhoods. In this context, the
               comparison of the ranks in the high-and low-dimensional spaces
               leads to the definition of the co-ranking matrix. Rank errors
               and concepts such as neighborhood …",
  publisher = "jmlr.org",
  pages     = "21--35",
  year      =  2008
}



@BOOK{Lee2007-wq,
  title     = "Nonlinear Dimensionality Reduction",
  author    = "Lee, John A and Verleysen, Michel",
  abstract  = "Methods of dimensionality reduction provide a way to understand
               and visualize the structure of complex data sets. Traditional
               methods like principal component analysis and classical metric
               multidimensional scaling suffer from being based on linear
               models. Until recently, very few methods were able to reduce the
               data dimensionality in a nonlinear way. However, since the late
               nineties, many new methods have been developed and nonlinear
               dimensionality reduction, also called manifold learning, has
               become a hot topic. New advances that account for this rapid
               growth are, e.g. the use of graphs to represent the manifold
               topology, and the use of new metrics like the geodesic distance.
               In addition, new optimization schemes, based on kernel
               techniques and spectral decomposition, have lead to spectral
               embedding, which encompasses many of the recently developed
               methods. This book describes existing and advanced methods to
               reduce the dimensionality of numerical databases. For each
               method, the description starts from intuitive ideas, develops
               the necessary mathematical details, and ends by outlining the
               algorithmic implementation. Methods are compared with each other
               with the help of different illustrative examples. The purpose of
               the book is to summarize clear facts and ideas about well-known
               methods as well as recent developments in the topic of nonlinear
               dimensionality reduction. With this goal in mind, methods are
               all described from a unifying point of view, in order to
               highlight their respective strengths and shortcomings. The book
               is primarily intended for statisticians, computer scientists and
               data analysts. It is also accessible to other practitioners
               having a basic background in statistics and/or computational
               learning, like psychologists (in psychometry) and economists.",
  publisher = "Springer Science \& Business Media",
  month     =  oct,
  year      =  2007,
  language  = "en"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Lee2008-sx,
  title     = "Rank-based quality assessment of nonlinear dimensionality
               reduction",
  booktitle = "{ESANN}",
  author    = "Lee, John Aldo and Verleysen, Michel and {Others}",
  abstract  = "Nonlinear dimensionality reduction aims at providing
               lowdimensional representions of high- dimensional data sets.
               Many new methods have been proposed in the recent years, but the
               question of their assessment and comparison remains open. This
               paper reviews some of the existing quality measures that are
               based on distance ranking and K-ary neighborhoods. Many quality
               criteria actually rely on the analysis of one or several
               sub-blocks of a co- ranking matrix. The analogy between the
               co-ranking matrix and a Shepard diagram is …",
  publisher = "elen.ucl.ac.be",
  pages     = "49--54",
  year      =  2008
}



@ARTICLE{Lueks2011-oa,
  title         = "How to Evaluate Dimensionality Reduction? - Improving the
                   Co-ranking Matrix",
  author        = "Lueks, Wouter and Mokbel, Bassam and Biehl, Michael and
                   Hammer, Barbara",
  abstract      = "The growing number of dimensionality reduction methods
                   available for data visualization has recently inspired the
                   development of quality assessment measures, in order to
                   evaluate the resulting low-dimensional representation
                   independently from a methods' inherent criteria. Several
                   (existing) quality measures can be (re)formulated based on
                   the so-called co-ranking matrix, which subsumes all rank
                   errors (i.e. differences between the ranking of distances
                   from every point to all others, comparing the
                   low-dimensional representation to the original data). The
                   measures are often based on the partioning of the co-ranking
                   matrix into 4 submatrices, divided at the K-th row and
                   column, calculating a weighted combination of the sums of
                   each submatrix. Hence, the evaluation process typically
                   involves plotting a graph over several (or even all
                   possible) settings of the parameter K. Considering simple
                   artificial examples, we argue that this parameter controls
                   two notions at once, that need not necessarily be combined,
                   and that the rectangular shape of submatrices is
                   disadvantageous for an intuitive interpretation of the
                   parameter. We debate that quality measures, as general and
                   flexible evaluation tools, should have parameters with a
                   direct and intuitive interpretation as to which specific
                   error types are tolerated or penalized. Therefore, we
                   propose to replace K with two parameters to control these
                   notions separately, and introduce a differently shaped
                   weighting on the co-ranking matrix. The two new parameters
                   can then directly be interpreted as a threshold up to which
                   rank errors are tolerated, and a threshold up to which the
                   rank-distances are significant for the evaluation. Moreover,
                   we propose a color representation of local quality to
                   visually support the evaluation process for a given mapping,
                   where every point in the mapping is colored according to its
                   local contribution to the overall quality.",
  month         =  oct,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1110.3917"
}



@ARTICLE{Lee2015-tc,
  title     = "Multi-scale similarities in stochastic neighbour embedding:
               Reducing dimensionality while preserving both local and global
               structure",
  author    = "Lee, John A and Peluffo-Ord{\'o}{\~n}ez, Diego H and Verleysen,
               Michel",
  abstract  = "Stochastic neighbour embedding (SNE) and its variants are
               methods of nonlinear dimensionality reduction that involve soft
               Gaussian neighbourhoods to measure similarities for all pairs of
               data. In order to build a suitable embedding, these methods try
               to reproduce in a low-dimensional space the neighbourhoods that
               are observed in the high-dimensional data space. Previous works
               have investigated the immunity of such similarities to norm
               concentration, as well as enhanced cost functions, like sums of
               Jensen--Shannon divergences. This paper proposes an additional
               refinement, namely multi-scale similarities, which are averages
               of soft Gaussian neighbourhoods with exponentially growing
               bandwidths. Such multi-scale similarities can replace the
               regular, single-scale neighbourhoods in SNE-like methods. Their
               objective is then to maximise the embedding quality on all
               scales, with the best preservation of both local and global
               neighbourhoods, and also to exempt the user from having to fix a
               scale arbitrarily. Experiments with several data sets show that
               the proposed multi-scale approach captures better the structure
               of data and improves significantly the quality of dimensionality
               reduction.",
  journal   = "Neurocomputing",
  publisher = "Elsevier",
  volume    =  169,
  pages     = "246--261",
  month     =  dec,
  year      =  2015,
  keywords  = "Nonlinear dimensionality reduction; Manifold learning; Data
               visualisation; Jensen--Shannon divergence; Stochastic neighbour
               embedding"
}


@article{RJ-2018-039,
  author = {Guido Kraemer and Markus Reichstein and Miguel D. Mahecha},
  title = {{dimRed and coRanking---Unifying Dimensionality Reduction in R}},
  year = {2018},
  journal = {{The R Journal}},
  url = {https://journal.r-project.org/archive/2018/RJ-2018-039/index.html},
  pages = {342--358},
  volume = {10},
  number = {1}
}

@article{lecam1973,
  title={Convergence of estimates under dimensionality restrictions},
  author={LeCam, Lucien},
  journal={The Annals of Statistics},
  volume={1},
  number={1},
  pages={38--53},
  year={1973},
  publisher={Institute of Mathematical Statistics}
}

@ARTICLE{Cook1995,
  title     = "Grand tour and projection pursuit",
  author    = "Cook, Dianne and Buja, Andreas and Cabrera, Javier and Catherine Hurley",
  journal   = "J Computational and Graphical Statistics",
  year      =  1995,
  volume = 4,
  issue = 3,
  pages = {155-172}
}


@ARTICLE{Laa2020,
  title     = "A slice tour for finding hollowness in high-dimensional data",
  author    = "Laa, Ursula and Cook, Dianne and Valencia, German",
  journal   = "Journal of Computational and",
  year      =  2020,
  volume = 29,
  issue = 3,
  pages = {681-687},
}

@article{Talagala2020,
  author = {Priyanga Dilini Talagala and Rob J. Hyndman and Kate Smith-Miles and Sevvandi Kandanaarachchi and Mario A. Muñoz},
  title = {Anomaly Detection in Streaming Nonstationary Temporal Data},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {1},
  pages = {13-27},
  year  = {2020},
}

@book{amari2016,
  title={Information geometry and its applications},
  author={Amari, Shun-ichi},
  volume={194},
  year={2016},
  publisher={Springer}
}

@inproceedings{lee2007,
  title={Dimensionality reduction and clustering on statistical manifolds},
  author={Lee, Sang-Mook and Abbott, A Lynn and Araman, Philip A},
  booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--7},
  year={2007},
  organization={IEEE}
}

@article{carter2009,
  title={Fine: Fisher information nonparametric embedding},
  author={Carter, Kevin M and Raich, Raviv and Finn, William G and Hero III, Alfred O},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={31},
  number={11},
  pages={2093--2098},
  year={2009},
  publisher={IEEE}
}
